{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Rewrite optModel and optDataset module\n",
    "2. Write a training loop\n",
    "3. Define regret and track regret/runtime\n",
    "4. Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d564cd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'E:\\\\User\\\\Stevens\\\\Spring 2024\\\\PTO - Fairness\\\\myGit\\\\myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optModel:\n",
    "    def __init__(self, x, r, c, Q, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.Q = Q\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.x = x\n",
    "        self.num_data, self.num_items, self.num_features = x.shape\n",
    "\n",
    "    def setObj(self, r, c):\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "    def solve(self, closed=False):\n",
    "        opt_sol = []\n",
    "        opt_val = []\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        for i in range(self.num_data):\n",
    "            self.d = cp.Variable(self.num_items)\n",
    "            self.setObj(self.r[i], self.c[i])\n",
    "            self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "            opt_sol.append(self.d.value.reshape(1, self.num_items))\n",
    "            opt_val.append(self.problem.value)\n",
    "\n",
    "        opt_sol = np.concatenate(opt_sol)\n",
    "        return opt_sol, opt_val\n",
    "\n",
    "    def solveC(self):\n",
    "        if self.alpha == 1:\n",
    "            return \"Work in progress\"\n",
    "        \n",
    "        opt_sols_c = []\n",
    "        opt_vals_c = []\n",
    "        for i in range(self.num_data):\n",
    "            S = np.sum(self.c[i] ** (1 - 1 / self.alpha) * self.r[i] ** (-1 + 1 / self.alpha))\n",
    "            opt_sol_c = (self.c[i] ** (-1 / self.alpha) * self.r[i] ** (-1 + 1 / self.alpha) * self.Q) / S\n",
    "            opt_val_c = np.sum((self.r[i] * opt_sol_c) ** (1 - self.alpha)) / (1 - self.alpha)\n",
    "            opt_sols_c.append(opt_sol_c)\n",
    "            opt_vals_c.append(opt_val_c)\n",
    "        \n",
    "        opt_sols_c = np.array(opt_sols_c)\n",
    "        return opt_sols_c, opt_vals_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    x = rnd.normal(0, 1, (n, m, p))\n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    r = rnd.normal(0, 1, (n, m))\n",
    "    r = 1 / (1 + np.exp(-r))\n",
    "\n",
    "    return x, r, c, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [1.9280332  4.93743494 0.7083329  2.73212343 3.71207079 1.47603921\n",
      " 0.35599286 0.37031601 0.32396207 1.71644314]\n",
      "Objective value: 15.199442184027435\n",
      "Optimal solution (closed form): [1.92803807 4.93741623 0.70833192 2.7321237  3.71205201 1.47604698\n",
      " 0.35599551 0.37031548 0.32396474 1.71644999]\n",
      "Objective value (closed form): 15.199442184254428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test optModel with synthetic data\n",
    "x, r, c, Q = genData(2000, 20, 10)\n",
    "alpha = 0.5\n",
    "\n",
    "# Create an instance of the optModel class\n",
    "optmodel = optModel(x, r, c, Q, alpha)\n",
    "\n",
    "# Solve the optimization problem\n",
    "opt_sol, opt_val = optmodel.solve()\n",
    "\n",
    "print(\"Optimal solution:\", opt_sol[0])\n",
    "print(\"Objective value:\", opt_val[0])\n",
    "\n",
    "opt_sol_c, opt_val_c = optmodel.solve(closed=True)\n",
    "\n",
    "print(\"Optimal solution (closed form):\", opt_sol_c[0])\n",
    "print(\"Objective value (closed form):\", opt_val_c[0])\n",
    "\n",
    "# Are they the same?\n",
    "np.allclose(opt_sol, opt_sol_c, atol=1e-4, rtol=1e-4)\n",
    "np.allclose(opt_val, opt_val_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试optDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optDataset(Dataset):\n",
    "    def __init__(self, x, r, c, Q, alpha, closed=False):\n",
    "        self.closed = closed\n",
    "        self.x = x\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data, self.num_items, self.num_features = x.shape\n",
    "\n",
    "        self._solve_optimization_problems()\n",
    "\n",
    "    def _solve_optimization_problems(self):\n",
    "        optmodel = optModel(self.x, self.r, self.c, self.Q, self.alpha)\n",
    "        self.opt_sols, self.opt_vals = optmodel.solve()\n",
    "        self.opt_sols_c, self.opt_vals_c = optmodel.solve(closed=True)\n",
    "        self.opt_vals = np.array(self.opt_vals)\n",
    "        self.opt_vals_c = np.array(self.opt_vals_c)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.r)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret = (\n",
    "                torch.tensor(self.x[idx]),\n",
    "                torch.tensor(self.r[idx]),\n",
    "                torch.tensor(self.c[idx]),\n",
    "                torch.tensor(self.opt_sols[idx]),\n",
    "                torch.tensor(self.opt_vals[idx]),\n",
    "                \n",
    "            )\n",
    "        if self.closed:\n",
    "            ret = ret + (torch.tensor(self.opt_sols_c[idx]),\n",
    "                torch.tensor(self.opt_vals_c[idx]),\n",
    "    )                \n",
    "        return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4967, -0.1383,  0.6477,  1.5230, -0.2342, -0.2341,  1.5792,  0.7674,\n",
       "          -0.4695,  0.5426, -0.4634, -0.4657,  0.2420, -1.9133, -1.7249, -0.5623,\n",
       "          -1.0128,  0.3142, -0.9080, -1.4123],\n",
       "         [ 1.4656, -0.2258,  0.0675, -1.4247, -0.5444,  0.1109, -1.1510,  0.3757,\n",
       "          -0.6006, -0.2917, -0.6017,  1.8523, -0.0135, -1.0577,  0.8225, -1.2208,\n",
       "           0.2089, -1.9597, -1.3282,  0.1969],\n",
       "         [ 0.7385,  0.1714, -0.1156, -0.3011, -1.4785, -0.7198, -0.4606,  1.0571,\n",
       "           0.3436, -1.7630,  0.3241, -0.3851, -0.6769,  0.6117,  1.0310,  0.9313,\n",
       "          -0.8392, -0.3092,  0.3313,  0.9755],\n",
       "         [-0.4792, -0.1857, -1.1063, -1.1962,  0.8125,  1.3562, -0.0720,  1.0035,\n",
       "           0.3616, -0.6451,  0.3614,  1.5380, -0.0358,  1.5646, -2.6197,  0.8219,\n",
       "           0.0870, -0.2990,  0.0918, -1.9876],\n",
       "         [-0.2197,  0.3571,  1.4779, -0.5183, -0.8085, -0.5018,  0.9154,  0.3288,\n",
       "          -0.5298,  0.5133,  0.0971,  0.9686, -0.7021, -0.3277, -0.3921, -1.4635,\n",
       "           0.2961,  0.2611,  0.0051, -0.2346],\n",
       "         [-1.4154, -0.4206, -0.3427, -0.8023, -0.1613,  0.4041,  1.8862,  0.1746,\n",
       "           0.2576, -0.0744, -1.9188, -0.0265,  0.0602,  2.4632, -0.1924,  0.3015,\n",
       "          -0.0347, -1.1687,  1.1428,  0.7519],\n",
       "         [ 0.7910, -0.9094,  1.4028, -1.4019,  0.5869,  2.1905, -0.9905, -0.5663,\n",
       "           0.0997, -0.5035, -1.5507,  0.0686, -1.0623,  0.4736, -0.9194,  1.5499,\n",
       "          -0.7833, -0.3221,  0.8135, -1.2309],\n",
       "         [ 0.2275,  1.3071, -1.6075,  0.1846,  0.2599,  0.7818, -1.2370, -1.3205,\n",
       "           0.5219,  0.2970,  0.2505,  0.3464, -0.6800,  0.2323,  0.2931, -0.7144,\n",
       "           1.8658,  0.4738, -1.1913,  0.6566],\n",
       "         [-0.9747,  0.7871,  1.1586, -0.8207,  0.9634,  0.4128,  0.8221,  1.8968,\n",
       "          -0.2454, -0.7537, -0.8895, -0.8158, -0.0771,  0.3412,  0.2767,  0.8272,\n",
       "           0.0130,  1.4535, -0.2647,  2.7202],\n",
       "         [ 0.6257, -0.8572, -1.0709,  0.4825, -0.2235,  0.7140,  0.4732, -0.0728,\n",
       "          -0.8468, -1.5148, -0.4465,  0.8564,  0.2141, -1.2457,  0.1732,  0.3853,\n",
       "          -0.8839,  0.1537,  0.0582, -1.1430]], dtype=torch.float64),\n",
       " tensor([0.5456, 0.4563, 0.1473, 0.3945, 0.5360, 0.6905, 0.1007, 0.1732, 0.1197,\n",
       "         0.2478], dtype=torch.float64),\n",
       " tensor([7., 4., 6., 5., 5., 9., 7., 9., 8., 5.], dtype=torch.float64),\n",
       " tensor([1.9280, 4.9374, 0.7083, 2.7321, 3.7121, 1.4760, 0.3560, 0.3703, 0.3240,\n",
       "         1.7164], dtype=torch.float64),\n",
       " tensor(15.1994, dtype=torch.float64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test optDatasetRd\n",
    "dataset = optDataset(x, r, c, Q, alpha)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 10\n",
      "Number of features: 20\n"
     ]
    }
   ],
   "source": [
    "num_items = x.shape[1]\n",
    "num_features = x.shape[2]\n",
    "print(\"Number of items:\", num_items)\n",
    "print(\"Number of features:\", num_features)\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, num_items, num_features):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_features, 1) for _ in range(num_items)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_items):\n",
    "            outputs.append(torch.sigmoid(self.linears[i](x[:, i, :])))\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRegret(optmodel, x, true_c, pred_r, true_r, true_obj, alpha):\n",
    "    model = optmodel(x,pred_r, true_c, Q, alpha)\n",
    "    sol, _ = model.solve()\n",
    "    val = []\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = np.sum((true_r[i] * sol[i]) ** (1 - alpha)) / (1 - alpha)\n",
    "        val.append(temp)\n",
    "    val = torch.tensor(np.array(val)).to(device)\n",
    "    regret_loss = 0\n",
    "    for i in range(x.shape[1]):\n",
    "        regret_loss += true_obj[i] - val[i]\n",
    "    return regret_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LogisticRegressionModel(num_items, num_features).to(device)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 5.9829, Time: 5.21 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 5.9356, Time: 5.18 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 5.9759, Time: 5.29 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 5.9514, Time: 5.53 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 6.2289, Time: 5.46 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 5.9794, Time: 5.29 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 5.8387, Time: 5.43 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-9): 10 x Linear(in_features=20, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     21\u001b[0m     pred_r \u001b[38;5;241m=\u001b[39m model(x_batch)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 23\u001b[0m regret_loss \u001b[38;5;241m=\u001b[39m calRegret(optModel, x_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), c_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), pred_r, r_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), opt_vals_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), alpha)\n\u001b[0;32m     24\u001b[0m loss_log\u001b[38;5;241m.\u001b[39mappend(regret_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     25\u001b[0m regret_loss\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mcalRegret\u001b[1;34m(optmodel, x, true_c, pred_r, true_r, true_obj, alpha)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalRegret\u001b[39m(optmodel, x, true_c, pred_r, true_r, true_obj, alpha):\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m optmodel(x,pred_r, true_c, Q, alpha)\n\u001b[1;32m----> 3\u001b[0m     sol, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39msolve()\n\u001b[0;32m      4\u001b[0m     val \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m, in \u001b[0;36moptModel.solve\u001b[1;34m(self, closed)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetObj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc[i])\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem\u001b[38;5;241m.\u001b[39msolve(abstol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m, reltol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m, feastol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[0;32m     32\u001b[0m opt_sol\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_items))\n\u001b[0;32m     33\u001b[0m opt_val\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\cvxpy\\problems\\problem.py:503\u001b[0m, in \u001b[0;36mProblem.solve\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     solve_func \u001b[38;5;241m=\u001b[39m Problem\u001b[38;5;241m.\u001b[39m_solve\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solve_func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\cvxpy\\problems\\problem.py:1082\u001b[0m, in \u001b[0;36mProblem._solve\u001b[1;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[0;32m   1078\u001b[0m     s\u001b[38;5;241m.\u001b[39mLOGGER\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1079\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvoking solver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m  to obtain a solution.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1080\u001b[0m             solving_chain\u001b[38;5;241m.\u001b[39mreductions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname())\n\u001b[0;32m   1081\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1082\u001b[0m solution \u001b[38;5;241m=\u001b[39m solving_chain\u001b[38;5;241m.\u001b[39msolve_via_data(\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m, data, warm_start, verbose, kwargs)\n\u001b[0;32m   1084\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_time \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:453\u001b[0m, in \u001b[0;36mSolvingChain.solve_via_data\u001b[1;34m(self, problem, data, warm_start, verbose, solver_opts)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msolve_via_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, problem, data, warm_start: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    418\u001b[0m                    solver_opts\u001b[38;5;241m=\u001b[39m{}):\n\u001b[0;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Solves the problem using the data output by the an apply invocation.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m    The semantics are:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m        a Solution object.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver\u001b[38;5;241m.\u001b[39msolve_via_data(data, warm_start, verbose,\n\u001b[0;32m    454\u001b[0m                                       solver_opts, problem\u001b[38;5;241m.\u001b[39m_solver_cache)\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\cvxpy\\reductions\\solvers\\conic_solvers\\ecos_conif.py:137\u001b[0m, in \u001b[0;36mECOS.solve_via_data\u001b[1;34m(self, data, warm_start, verbose, solver_opts, solver_cache)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data[s\u001b[38;5;241m.\u001b[39mA] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data[s\u001b[38;5;241m.\u001b[39mA]\u001b[38;5;241m.\u001b[39mnnz \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39mprod(data[s\u001b[38;5;241m.\u001b[39mA]\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mECOS cannot handle sparse data with nnz == 0; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis is a bug in ECOS, and it indicates that your problem \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have redundant constraints.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m solution \u001b[38;5;241m=\u001b[39m ecos\u001b[38;5;241m.\u001b[39msolve(data[s\u001b[38;5;241m.\u001b[39mC], data[s\u001b[38;5;241m.\u001b[39mG], data[s\u001b[38;5;241m.\u001b[39mH],\n\u001b[0;32m    138\u001b[0m                       cones, data[s\u001b[38;5;241m.\u001b[39mA], data[s\u001b[38;5;241m.\u001b[39mB],\n\u001b[0;32m    139\u001b[0m                       verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    140\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msolver_opts)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solution\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\ecos\\ecos.py:60\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(c, G, h, dims, A, b, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     data, indices, colptr \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39mdata, G\u001b[38;5;241m.\u001b[39mindices, G\u001b[38;5;241m.\u001b[39mindptr\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ecos\u001b[38;5;241m.\u001b[39mcsolve((m,n1,p), c, data, indices, colptr, h, dims, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ecos\u001b[38;5;241m.\u001b[39mcsolve((m,n1,p), c, data, indices, colptr, h, dims, A\u001b[38;5;241m.\u001b[39mdata, A\u001b[38;5;241m.\u001b[39mindices, A\u001b[38;5;241m.\u001b[39mindptr, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "loss_log = []\n",
    "num_epochs = 20\n",
    "total_time = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for x_batch, r_batch, c_batch, opt_sols_batch, opt_vals_batch in loader_train:\n",
    "        \n",
    "        x_batch = x_batch.float().to(device)\n",
    "        r_batch = r_batch.float().to(device)\n",
    "        c_batch = c_batch.float().to(device)\n",
    "        opt_vals_batch = opt_vals_batch.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            pred_r = model(x_batch).to(\"cpu\").detach().numpy()\n",
    "        model.train()\n",
    "\n",
    "        regret_loss = calRegret(optModel, x_batch.detach().cpu().numpy(), c_batch.detach().cpu().numpy(), pred_r, r_batch.detach().cpu().numpy(), opt_vals_batch.detach().cpu().numpy(), alpha)\n",
    "        loss_log.append(regret_loss.detach().cpu().numpy())\n",
    "        regret_loss.requires_grad = True\n",
    "        loss = regret_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    total_time += epoch_time\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(loader_train):.4f}, Time: {epoch_time:.2f} seconds')\n",
    "\n",
    "average_time = total_time / num_epochs\n",
    "print(f'Average time per epoch: {average_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.470971254232026,\n",
       " 6.253414408137411,\n",
       " 3.8935260356906873,\n",
       " 6.498943766432147,\n",
       " 4.891093938310952,\n",
       " 7.357825899939346,\n",
       " 6.310745932094596,\n",
       " 5.974428944175525,\n",
       " 5.836165131583201,\n",
       " 6.036031089472701,\n",
       " 6.371932788066573,\n",
       " 5.815670852821821,\n",
       " 5.302628063239279,\n",
       " 6.815153269219461,\n",
       " 5.2577811307324325,\n",
       " 6.247972097209928,\n",
       " 6.5235526635166075,\n",
       " 5.043136293015888,\n",
       " 6.208393732026954,\n",
       " 5.444309903978482,\n",
       " 6.589620457080075,\n",
       " 6.344759942165673,\n",
       " 4.169085566965894,\n",
       " 5.8418538027017775,\n",
       " 5.810741535543551,\n",
       " 6.754523773001793,\n",
       " 5.4387167433155525,\n",
       " 6.052933588500961,\n",
       " 5.48775395783176,\n",
       " 5.502638415606498,\n",
       " 6.738109818405286,\n",
       " 4.8225387398826385,\n",
       " 5.312491039840143,\n",
       " 6.9288674280181155,\n",
       " 6.296307722805739,\n",
       " 5.689088394563553,\n",
       " 4.515608725313047,\n",
       " 6.319262309308609,\n",
       " 6.13336718205743,\n",
       " 4.209573468799022,\n",
       " 6.618216962026821,\n",
       " 5.546156666944176,\n",
       " 8.005099778065837,\n",
       " 6.4688995236131035,\n",
       " 6.357097649071841,\n",
       " 5.384003341605375,\n",
       " 4.933188079827325,\n",
       " 5.434968191879406,\n",
       " 6.046880221725141,\n",
       " 4.450310192627651,\n",
       " 8.442834761148596,\n",
       " 5.825072685987546,\n",
       " 5.541349385261617,\n",
       " 4.910576265984055,\n",
       " 5.942209130451394,\n",
       " 6.046233103493165,\n",
       " 5.206740045146226,\n",
       " 4.8805948846368175,\n",
       " 5.034556891823483,\n",
       " 6.175617064497239,\n",
       " 7.97086303055095,\n",
       " 5.924846259176203,\n",
       " 5.209123244811085,\n",
       " 4.367322177166169,\n",
       " 6.529778395229464,\n",
       " 5.14735615318051,\n",
       " 5.255025797383286,\n",
       " 7.817112926228214,\n",
       " 5.4790332457141595,\n",
       " 5.28291597841357,\n",
       " 5.916736654823371,\n",
       " 5.3021433839671275,\n",
       " 6.0085925133251,\n",
       " 6.390360759392546,\n",
       " 5.924195388347837,\n",
       " 6.882885831668888,\n",
       " 6.2221646392806775,\n",
       " 4.864611328078375,\n",
       " 4.704109496703863,\n",
       " 7.014768460977029,\n",
       " 5.469580236865818,\n",
       " 6.31613601904197,\n",
       " 5.538440388699371,\n",
       " 4.721037032838584,\n",
       " 7.038347142865097,\n",
       " 6.46639877588815,\n",
       " 7.431337605754258,\n",
       " 5.574280178186047,\n",
       " 6.3258191394629755,\n",
       " 5.455662126564624,\n",
       " 6.075295497710442,\n",
       " 5.397545878369961,\n",
       " 5.695086314426362,\n",
       " 5.43142082934143,\n",
       " 6.682798566653144,\n",
       " 5.590745776091591,\n",
       " 6.311368624842505,\n",
       " 6.185142493408458,\n",
       " 6.912612533853778,\n",
       " 7.607111305029802,\n",
       " 7.266123509458353,\n",
       " 6.571462465358941,\n",
       " 5.548390002908796,\n",
       " 5.927747460061319,\n",
       " 5.204775821530751,\n",
       " 6.531162177757787,\n",
       " 5.379769349687557,\n",
       " 5.194168558592899,\n",
       " 5.529583570096525,\n",
       " 5.588498327086489,\n",
       " 6.198552225818634,\n",
       " 6.182810120844824,\n",
       " 4.699333646595491,\n",
       " 5.596359517410258,\n",
       " 4.767111506455894,\n",
       " 4.844456967303941,\n",
       " 7.215234770573852,\n",
       " 4.741515568723189,\n",
       " 6.734238847445496,\n",
       " 5.404986384429872,\n",
       " 5.184884380434703,\n",
       " 6.500239607795528,\n",
       " 5.704988295604402,\n",
       " 6.227166727782102,\n",
       " 4.836546392878695,\n",
       " 6.014693540318271,\n",
       " 4.954070654751687,\n",
       " 5.7153940902595295,\n",
       " 6.768139238687537,\n",
       " 7.120054812950279,\n",
       " 6.9706897159568015,\n",
       " 6.583665224441532,\n",
       " 7.0540594721944,\n",
       " 7.918226067430776,\n",
       " 4.650987290271498,\n",
       " 5.593902423364966,\n",
       " 5.705015393759206,\n",
       " 5.522261294997822,\n",
       " 5.001508445202278,\n",
       " 6.143627469590804,\n",
       " 4.903626962547797,\n",
       " 4.6604040485904115,\n",
       " 6.364675967598156,\n",
       " 5.4171395274310115,\n",
       " 6.244847602671008,\n",
       " 4.641827193135288,\n",
       " 5.835647242048545,\n",
       " 6.045331038892588,\n",
       " 4.4761495194016305,\n",
       " 5.331870043055856,\n",
       " 6.8864373929232965,\n",
       " 4.847357786102233,\n",
       " 6.12140131604475,\n",
       " 4.7901041922507925,\n",
       " 5.842106431915022,\n",
       " 7.276435617143793,\n",
       " 5.78700579750036,\n",
       " 6.609210249654199,\n",
       " 5.405418030242334,\n",
       " 6.6051864031122935,\n",
       " 5.975515462730359,\n",
       " 5.286829450463619,\n",
       " 7.071318551306746,\n",
       " 7.087740705646606,\n",
       " 5.868420604075661,\n",
       " 5.5376531447818405,\n",
       " 6.663731304081555,\n",
       " 4.7966794617486315,\n",
       " 6.073496722467951,\n",
       " 5.960553074065224,\n",
       " 6.624433193324709,\n",
       " 6.151846779759808,\n",
       " 6.185256677078273,\n",
       " 6.617408336676293,\n",
       " 5.5093565254120875,\n",
       " 6.611323825945323,\n",
       " 6.13087288404623,\n",
       " 6.247595589751965,\n",
       " 5.962438876544752,\n",
       " 6.280709680216571,\n",
       " 4.8842036870561785,\n",
       " 6.1537879102655175,\n",
       " 5.7300355757150925,\n",
       " 6.693953643949541,\n",
       " 5.242877751198485,\n",
       " 4.980908064591702,\n",
       " 5.581989668671323,\n",
       " 7.602252963704611,\n",
       " 4.401198247010585,\n",
       " 7.142284008320457,\n",
       " 5.751876164896711,\n",
       " 6.020800451947654,\n",
       " 6.318440951893102,\n",
       " 7.3305635503594395,\n",
       " 6.58336822055905,\n",
       " 6.332562981858768,\n",
       " 5.742457548025445,\n",
       " 6.453468451822847,\n",
       " 6.65510163702576,\n",
       " 5.176897524318697,\n",
       " 4.828560573735992,\n",
       " 5.895613171981754,\n",
       " 5.398731295594413,\n",
       " 7.26687668354109,\n",
       " 4.780102262432099,\n",
       " 3.856296738967483,\n",
       " 6.13552226014964,\n",
       " 4.874238388166532,\n",
       " 5.875865799624915,\n",
       " 6.1550835492041145,\n",
       " 5.245482411337768,\n",
       " 6.683680690428329,\n",
       " 6.234365286361946,\n",
       " 5.506514882635679,\n",
       " 5.811641437363484,\n",
       " 4.810375173471751,\n",
       " 5.748678601819746,\n",
       " 6.964019336409619,\n",
       " 5.407062894960546,\n",
       " 7.600140594168975,\n",
       " 6.54214561920295,\n",
       " 6.097144629099951,\n",
       " 5.227776982794712,\n",
       " 5.925717260648021,\n",
       " 5.846521883710993,\n",
       " 5.3177863329968496,\n",
       " 9.355251454358175,\n",
       " 5.485138473378328,\n",
       " 4.1080247359532365,\n",
       " 5.349920196441332,\n",
       " 6.648017065929272,\n",
       " 5.454714163211587,\n",
       " 5.909503175839696,\n",
       " 5.210730509310213,\n",
       " 5.361938481093246,\n",
       " 5.480096187826689,\n",
       " 6.042754089430378,\n",
       " 4.612888471357209,\n",
       " 6.257007012967032,\n",
       " 5.9642336974443175,\n",
       " 6.387621356137306,\n",
       " 6.623102910813735,\n",
       " 4.8466912985132815,\n",
       " 4.5385825650059015,\n",
       " 6.396102292939567,\n",
       " 5.085407818551502,\n",
       " 6.121863246527651,\n",
       " 5.8567500168676645,\n",
       " 6.994899608832089,\n",
       " 6.303986428604981,\n",
       " 5.587271261746981,\n",
       " 6.588866021061616,\n",
       " 6.049955915653214,\n",
       " 5.8787948700653185,\n",
       " 6.596870444288653,\n",
       " 7.653602534006694,\n",
       " 6.30733648110524,\n",
       " 5.2229487413326545,\n",
       " 5.181020453488562,\n",
       " 5.617836521626582,\n",
       " 5.47917929189391,\n",
       " 6.350141300336366,\n",
       " 6.248734794622395,\n",
       " 4.798639004272365,\n",
       " 6.205608849451274,\n",
       " 5.215402802196202,\n",
       " 5.736462757769722,\n",
       " 4.268035430515113,\n",
       " 5.158433508180419,\n",
       " 5.489898485938795,\n",
       " 8.10992682366102,\n",
       " 4.741807851093569,\n",
       " 5.739561538963107,\n",
       " 5.269808624204876,\n",
       " 6.063453530418318,\n",
       " 6.764586652234964,\n",
       " 6.323647472086389,\n",
       " 6.641238937200212,\n",
       " 5.1507389826989805,\n",
       " 6.148447484760981,\n",
       " 4.625915355437758,\n",
       " 5.37279635394645,\n",
       " 5.280593810726295,\n",
       " 5.5283975725982994,\n",
       " 6.808308411799802,\n",
       " 7.844987673424598,\n",
       " 6.792753505168774,\n",
       " 5.286460391016469,\n",
       " 5.287033444628253,\n",
       " 7.138297449479092,\n",
       " 5.982952545518943,\n",
       " 6.112244773855728,\n",
       " 6.21650388490672,\n",
       " 6.20516148842264,\n",
       " 7.5366540451259745,\n",
       " 5.650783141088365,\n",
       " 7.132896173639702,\n",
       " 5.001965059124963,\n",
       " 5.1929467833245955,\n",
       " 5.351747536707867,\n",
       " 6.0729080585713415,\n",
       " 4.866702687451642,\n",
       " 6.271109186345406,\n",
       " 7.367786051660477,\n",
       " 7.622697495644477,\n",
       " 7.377411421051068,\n",
       " 7.216656572296152,\n",
       " 5.370742817014781,\n",
       " 4.623594600445445,\n",
       " 7.176428607292319,\n",
       " 5.4626144775721315,\n",
       " 6.786959971142302,\n",
       " 7.213454374281103,\n",
       " 6.199056545685941,\n",
       " 5.8994881029269415,\n",
       " 6.1612756915182025,\n",
       " 4.975488849596701,\n",
       " 6.969919029539868,\n",
       " 7.514140117582059,\n",
       " 6.071458420382331,\n",
       " 7.113546443383907,\n",
       " 5.914573891720405,\n",
       " 5.10533420620753,\n",
       " 5.708025736377049,\n",
       " 5.877513523779442,\n",
       " 6.595723173790796,\n",
       " 6.248287671353157,\n",
       " 4.833461058904426,\n",
       " 5.938475827723337,\n",
       " 5.7641900577787535,\n",
       " 5.811020033554762,\n",
       " 7.02917007633204,\n",
       " 6.3494251199184895,\n",
       " 5.032366147925185,\n",
       " 5.690784170955904,\n",
       " 6.6282375175906925,\n",
       " 5.6664824569799634,\n",
       " 6.226846752465612,\n",
       " 5.481074474539486,\n",
       " 5.616491653141427,\n",
       " 5.393176031350622,\n",
       " 4.835110344749918,\n",
       " 5.566544610239365,\n",
       " 6.071011650120553,\n",
       " 5.330607121722096,\n",
       " 5.145561826230152,\n",
       " 6.740476162606573,\n",
       " 7.403016478312306,\n",
       " 4.762589808440065,\n",
       " 5.075439523812248,\n",
       " 5.275298568192088,\n",
       " 6.62092454761739,\n",
       " 4.530791210829975,\n",
       " 7.272835072204874,\n",
       " 6.357741273096238,\n",
       " 4.871504962557468,\n",
       " 6.022139612268795,\n",
       " 6.021219356061405,\n",
       " 7.710439450124106,\n",
       " 5.9841105398148855,\n",
       " 5.715618423471014,\n",
       " 4.968680625602671,\n",
       " 5.388293983075986,\n",
       " 6.325190996091383,\n",
       " 7.001044389298752,\n",
       " 4.384662506151773,\n",
       " 6.517583281190223,\n",
       " 5.823314306109246,\n",
       " 5.324813564960847,\n",
       " 6.040680254940996,\n",
       " 6.865410977745531,\n",
       " 4.654874607051035,\n",
       " 6.048651137759112,\n",
       " 4.576211929588766,\n",
       " 5.4215174608049495,\n",
       " 6.377396665383156,\n",
       " 6.668224638907338,\n",
       " 3.5350724779416343,\n",
       " 7.707029249420575,\n",
       " 5.4992862597404475,\n",
       " 4.221535872955556,\n",
       " 7.315800493849924,\n",
       " 7.47589076416827,\n",
       " 6.374036679886203,\n",
       " 4.95403710377823,\n",
       " 7.112765934502953,\n",
       " 5.046690758029264,\n",
       " 6.902764109511294,\n",
       " 5.895974060074954,\n",
       " 6.302717439883349,\n",
       " 5.293025132090367,\n",
       " 6.396232755751088,\n",
       " 6.338114815774043,\n",
       " 5.10294424092327,\n",
       " 6.632752307021864,\n",
       " 6.166673916133956,\n",
       " 5.85950942365762,\n",
       " 5.913543159809333,\n",
       " 6.771261398929614,\n",
       " 4.562182857523716,\n",
       " 5.815274095797953,\n",
       " 4.934919816547321,\n",
       " 4.95688547540686,\n",
       " 6.010469285054022,\n",
       " 7.0278859751398315,\n",
       " 4.92391118211437,\n",
       " 6.1837753188731135,\n",
       " 4.664011966298897,\n",
       " 5.009927626529123,\n",
       " 4.185433784129849,\n",
       " 5.552313045771863,\n",
       " 5.685485175104622,\n",
       " 5.861080846614255,\n",
       " 5.3692396441174814,\n",
       " 8.326458537093407,\n",
       " 5.2505280564279815,\n",
       " 7.348717470841802,\n",
       " 6.038732515422534,\n",
       " 5.410483262472486,\n",
       " 6.254030981260186,\n",
       " 6.548305662940717,\n",
       " 5.7350788504113765,\n",
       " 6.1171699850980055,\n",
       " 6.580400628236536,\n",
       " 5.619561480567633,\n",
       " 6.495612325294967,\n",
       " 6.374088860250948,\n",
       " 6.799564908963468,\n",
       " 5.873031938562843,\n",
       " 6.9722373475854535,\n",
       " 5.1630461758644195,\n",
       " 5.623516018944876,\n",
       " 6.01558145579547,\n",
       " 5.572639475398542,\n",
       " 5.713566250188862,\n",
       " 5.369024911639144,\n",
       " 6.056540545296366,\n",
       " 4.668624425034375,\n",
       " 6.582923931563311,\n",
       " 7.012946885953944,\n",
       " 6.437800008214815,\n",
       " 8.090037496648495,\n",
       " 5.499692089789722,\n",
       " 5.9108917257449995,\n",
       " 5.6247666909224705,\n",
       " 5.571183938779821,\n",
       " 6.559491128269038,\n",
       " 5.570340573577305,\n",
       " 5.514984575747018,\n",
       " 7.04352459879321,\n",
       " 4.820687321433567,\n",
       " 4.764588145350222,\n",
       " 5.783946720051468,\n",
       " 5.037329869458663,\n",
       " 6.492364172302375,\n",
       " 4.688779215291714,\n",
       " 4.7296695963995,\n",
       " 4.907476382218459,\n",
       " 6.941553246928603,\n",
       " 5.961683201743629,\n",
       " 6.456769083790466,\n",
       " 5.897382777784154,\n",
       " 5.739106340242316,\n",
       " 6.656843947574824,\n",
       " 6.173240541421137,\n",
       " 5.910735219366522,\n",
       " 5.869850161544694,\n",
       " 5.317824310811552,\n",
       " 7.202760479234138,\n",
       " 5.051885256879723,\n",
       " 5.599086453488541,\n",
       " 5.244728137496521,\n",
       " 6.019036226728563,\n",
       " 6.555759126288752,\n",
       " 5.794972488579766,\n",
       " 6.128710392186642,\n",
       " 6.9025910837073745,\n",
       " 4.187968236427839,\n",
       " 4.155330243643064,\n",
       " 4.5381397566510095,\n",
       " 5.3371108457079455,\n",
       " 5.362630619735308,\n",
       " 5.836733494924841,\n",
       " 5.627882847309369,\n",
       " 7.382573575907127,\n",
       " 6.382417862692696,\n",
       " 7.358357434517972,\n",
       " 6.453947043752244,\n",
       " 5.0158387084064096,\n",
       " 5.121182957626399,\n",
       " 6.983029149315186,\n",
       " 5.518545956848435,\n",
       " 4.9438213814593155,\n",
       " 5.879948732264257,\n",
       " 6.004453036536141,\n",
       " 6.174114696928594,\n",
       " 6.4544635274526545,\n",
       " 5.178201865759021,\n",
       " 6.497927773937038,\n",
       " 7.456952464889408]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = [i.tolist() for i in loss_log]\n",
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xbd2c1a10>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss Curve')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(demo)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(predmodel, optmodel, dataloader, closed=False, alpha=0.5):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # eval\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "\n",
    "    if not closed:\n",
    "    # load data\n",
    "        for data in dataloader:\n",
    "            x,r,c,opt_sol,opt_obj,_,_ = data\n",
    "            # cuda\n",
    "            x,r,c,opt_sol,opt_obj = x.cuda(),r.cuda(),c.cuda(),opt_sol.cuda(),opt_obj.cuda()\n",
    "            # predict\n",
    "            with torch.no_grad():\n",
    "                pred_r = predmodel(x).to('cpu').detach().numpy()\n",
    "            # solve\n",
    "            for j in range(pred_r.shape[0]):\n",
    "                loss += calRegret(optmodel, c[j], pred_r[j], r[j].to(\"cpu\").detach().numpy(), opt_obj[j].item(), alpha)\n",
    "\n",
    "                optsum += abs(opt_obj[j].item())\n",
    "    # turn back to train mode\n",
    "    predmodel.train()\n",
    "\n",
    "    # normalize\n",
    "    return loss / (optsum+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
