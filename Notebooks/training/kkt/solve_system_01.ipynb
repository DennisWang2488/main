{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on regret loss requires that we compute the derivative of the fairness problem with respect to its input parameters. \n",
    "\n",
    "We differentiate KKT conditions to obtain these derivatives.\n",
    "\n",
    "We use chain rule to compute the derivative of the problem w.r.t. its parameter from \n",
    "\\begin{align}\n",
    "    \\frac{\\partial{l_{Regret}(\\hat{r},\\cdot)}}{\\partial{\\boldsymbol\\theta}} = \\frac{\\partial{l_{Regret}(\\hat{r},\\cdot)}}{\\partial{d^*(\\hat{r})}} \\cdot \\frac{\\partial{d^*(\\hat{r})}}{\\partial{\\hat{r}}} \\cdot \\frac{\\partial{\\hat{r}}}{\\partial{\\boldsymbol{\\theta}}}\n",
    "\\end{align}) \n",
    "\n",
    "The derivative is calculated from solving the differentiated KKT system. \n",
    "\n",
    "i.e. $x = A^{-1} B$, here A is the invertible KKT matrix.\n",
    "\n",
    "What I need to code:\n",
    "- Replace the gradients calculation in the backward pass of regret loss training from pertubed to autograd.\n",
    "- For those closed form solutions, use the analytical gradient formula.\n",
    "- Comparison with basedline for Speed and for Loss performance:\n",
    "    - with two-stage\n",
    "    - for problems with closed-forms, compare with autograd for speed and for regret performance.\n",
    "    - for other regular problems, compare the matrix approach with autograd for speed and for regret performance.\n",
    "        - Here expect slower speed but slightly higher regret performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z:\n",
      " tensor([[2.1500, 2.9100],\n",
      "        [1.6000, 1.2600]], grad_fn=<AddmmBackward0>)\n",
      "gZ:\n",
      " tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "gX:\n",
      " tensor([[0.5000, 0.3000, 2.1000],\n",
      "        [0.2000, 0.1000, 1.1000]], requires_grad=True) tensor([[ 3.6000, -0.9000,  1.3000],\n",
      "        [ 3.6000, -0.9000,  1.3000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# function to extract grad\n",
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = grad * 2\n",
    "    return hook\n",
    "\n",
    "X = torch.tensor([[0.5, 0.3, 2.1], [0.2, 0.1, 1.1]], requires_grad=True)\n",
    "W = torch.tensor([[2.1, 1.5], [-1.4, 0.5], [0.2, 1.1]])\n",
    "B = torch.tensor([1.1, -0.3])\n",
    "\n",
    "# Z = XW^T + B\n",
    "Z = torch.nn.functional.linear(X, weight=W.t(), bias=B)\n",
    "\n",
    "# register_hook for Z\n",
    "Z.register_hook(set_grad(Z))\n",
    "\n",
    "S = torch.sum(Z)\n",
    "S.backward()\n",
    "print(\"Z:\\n\", Z)\n",
    "print(\"gZ:\\n\", Z.grad)\n",
    "print(\"gX:\\n\",X, X.grad)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:Notebooks/training/kkt/solve_system_01.ipynb
   "execution_count": 3,
=======
   "execution_count": 9,
>>>>>>> 97edc52a3ca6bbce3bdd53e77ff538a5ce500f28:Notebooks/training/solve_system_01.ipynb
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta:\n",
      " tensor([3.], requires_grad=True)\n",
      "dr/dtheta (should be cos(theta)):\n",
      " None\n",
      "g (r^2):\n",
      " tensor([0.0199], grad_fn=<PowBackward0>)\n",
      "Manually modified dg/dr:\n",
      " tensor([2.])\n",
      "Final gradient dL/dtheta:\n",
      " tensor([-0.2794])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:Notebooks/training/kkt/solve_system_01.ipynb
      "C:\\Users\\14469\\AppData\\Local\\Temp\\ipykernel_30476\\2141659903.py:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
=======
      "/var/folders/zm/yg4qm2yj0vs5j9t4_s31v5mw0000gn/T/ipykernel_60680/1987369378.py:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
>>>>>>> 97edc52a3ca6bbce3bdd53e77ff538a5ce500f28:Notebooks/training/solve_system_01.ipynb
      "  print(\"dr/dtheta (should be cos(theta)):\\n\", r.grad)\n"
     ]
    }
   ],
   "source": [
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = 2 * grad\n",
    "    return hook\n",
    "\n",
    "theta = torch.tensor([3.0], requires_grad=True)  # The initial variable theta\n",
    "r = torch.sin(theta)  # r = sin(theta)\n",
    "g = r ** 2  # g = r^2\n",
    "L = g.sum()  # L = g\n",
    "\n",
    "g.register_hook(set_grad(g))\n",
    "\n",
    "L.backward()\n",
    "\n",
    "print(\"theta:\\n\", theta)\n",
    "print(\"dr/dtheta (should be cos(theta)):\\n\", r.grad)\n",
    "print(\"g (r^2):\\n\", g)\n",
    "print(\"Manually modified dg/dr:\\n\", g.grad)\n",
    "print(\"Final gradient dL/dtheta:\\n\", theta.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of z with respect to x: tensor(4.)\n",
      "Gradient of z with respect to y: tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create input tensors\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define the computation\n",
    "z = x**2 + y**2\n",
    "\n",
    "# Compute gradients using autograd\n",
    "z.backward()\n",
    "\n",
    "# Access the gradients\n",
    "grad_x = x.grad\n",
    "grad_y = y.grad\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Gradient of z with respect to x:\", grad_x)\n",
    "print(\"Gradient of z with respect to y:\", grad_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_matrix(n):\n",
    "    \"\"\"\n",
    "    Generates a random nxn matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    n (int): The dimension of the matrix to be generated.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: A random nxn matrix.\n",
    "    \"\"\"\n",
    "    return np.random.rand(n, n)\n",
    "\n",
    "def solve_system_by_inversion(A, B):\n",
    "    \"\"\"\n",
    "    Solves the system Ax = B by matrix inversion.\n",
    "    \n",
    "    Parameters:\n",
    "    A (np.ndarray): The coefficient matrix.\n",
    "    B (np.ndarray): The right-hand side matrix or vector.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The solution vector or matrix x.\n",
    "    \"\"\"\n",
    "    # Check if the matrix A is invertible\n",
    "    if np.linalg.det(A) == 0:\n",
    "        raise ValueError(\"Matrix A is not invertible.\")\n",
    "    \n",
    "    # Calculate the inverse of A\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    \n",
    "    # Calculate the solution x\n",
    "    x = np.dot(A_inv, B)\n",
    "    \n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
<<<<<<< HEAD:Notebooks/training/kkt/solve_system_01.ipynb
      "[[0.37817492 0.63857121 0.92564704]\n",
      " [0.48543803 0.52120504 0.78655682]\n",
      " [0.89952628 0.40314126 0.04170855]]\n",
      "\n",
      "Matrix B:\n",
      "[[0.98917964]\n",
      " [0.96889453]\n",
      " [0.69563087]]\n",
      "\n",
      "Solution x:\n",
      "[[ 0.77175175]\n",
      " [-0.08013672]\n",
      " [ 0.80861868]]\n"
=======
      "[[0.22863226 0.21024465 0.49218183]\n",
      " [0.38962665 0.5420679  0.97965681]\n",
      " [0.60524233 0.26311156 0.98837755]]\n",
      "\n",
      "Matrix B:\n",
      "[[0.17933517]\n",
      " [0.68226605]\n",
      " [0.26231883]]\n",
      "\n",
      "Solution x:\n",
      "[[ 5.18830958]\n",
      " [ 5.37986154]\n",
      " [-4.34385573]]\n"
>>>>>>> 97edc52a3ca6bbce3bdd53e77ff538a5ce500f28:Notebooks/training/solve_system_01.ipynb
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "n = 3\n",
    "A = generate_matrix(n)\n",
    "B = np.random.rand(n, 1)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "try:\n",
    "    x = solve_system_by_inversion(A, B)\n",
    "    print(\"\\nSolution x:\")\n",
    "    print(x)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
