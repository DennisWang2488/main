{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'E:\\\\User\\\\Stevens\\\\Spring 2024\\\\PTO - Fairness\\\\myGit\\\\myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    # Split the population into group A (1/4) and group B (3/4)\n",
    "    group_A_size = n // 4\n",
    "    group_B_size = n - group_A_size\n",
    "    \n",
    "    # Generate x with a bias for group A\n",
    "    x = np.zeros((n, m, p))\n",
    "    x[:group_A_size] = rnd.normal(0.5, 1, (group_A_size, m, p))  # Slightly higher mean for group A\n",
    "    x[group_A_size:] = rnd.normal(0, 1, (group_B_size, m, p))   # Standard distribution for group B\n",
    "    \n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            \n",
    "            # Introduce bias for c: Group A has slightly higher values, Group B slightly lower\n",
    "            if i < group_A_size:\n",
    "                values *= 1.1  # Increase c for Group A\n",
    "            else:\n",
    "                values *= 0.9  # Decrease c for Group B\n",
    "            \n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    r = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            # Generate r using normal distribution, then clip to [0, 1] range\n",
    "            r[i, j] = np.clip(rnd.normal(0.5, 0.2), 0, 1)\n",
    "            \n",
    "            # Introduce bias for r: Group A has slightly lower values, Group B slightly higher\n",
    "            if i < group_A_size:\n",
    "                r[i, j] -= 0.1  # Decrease r for Group A\n",
    "            else:\n",
    "                r[i, j] += 0.1  # Increase r for Group B\n",
    "            \n",
    "            # Clip again to ensure values remain in [0, 1] range\n",
    "            r[i, j] = np.clip(r[i, j], 0, 1)\n",
    "\n",
    "    return x, r, c, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visLearningCurve(loss_log, loss_log_regret, mse_loss_log):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "    # Plot original loss log\n",
    "    ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve (Training Loss)\", fontsize=16)\n",
    "\n",
    "    # Plot regret log\n",
    "    ax2.plot(loss_log_regret, color=\"royalblue\", ls=\"--\", alpha=0.7, lw=2)\n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve (Test Regret)\", fontsize=16)\n",
    "\n",
    "    # Plot new MSE loss log\n",
    "    ax3.plot(mse_loss_log, color=\"orange\", lw=2)\n",
    "    ax3.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax3.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax3.set_ylabel(\"MSE Loss\", fontsize=16)\n",
    "    ax3.set_title(\"Learning Curve (MSE Loss)\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def objValue(d, r, alpha=0.5):\n",
    "    \"\"\"\n",
    "    A function to calculate objective value\n",
    "    \"\"\"\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(np.multiply(r, d)))\n",
    "    else:\n",
    "        return np.sum(np.power(np.multiply(r, d), 1 - alpha)) / (1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import solve\n",
    "\n",
    "# Helper function to check matrix invertibility\n",
    "def is_invertible(matrix):\n",
    "    return np.linalg.cond(matrix) < 1 / np.finfo(matrix.dtype).eps\n",
    "\n",
    "def solve_rd_matrix_system(r, c, d_i, mu, lambda_i, Q, alpha=0.5):\n",
    "    n = len(r)\n",
    "    \n",
    "    # Step 1: Construct Hessian of the objective function (H_dd)\n",
    "    A_elements = [alpha * r[i]**2 * (r[i] * d_i[i])**(-alpha - 1) for i in range(n)]\n",
    "    H_dd = np.diag(A_elements)\n",
    "    \n",
    "    # Step 2: Construct Jacobians of inequality constraints (g(d;r)) and equality constraints (h(d;r))\n",
    "    B = -np.eye(n)  # Jacobian of g(d;r) = d >= 0 is -I\n",
    "    C = c.reshape(-1, 1)  # Jacobian of h(d;r) = sum(c * d) - Q is the vector of c_i's\n",
    "    \n",
    "    # Step 3: Construct D and M matrices\n",
    "    D = np.diag(d_i)  # Diagonal matrix with d_i's\n",
    "    M = mu * c.reshape(1, -1)  # Row vector of mu * c\n",
    "    \n",
    "    # Step 4: Complementary slackness matrix with lambda_i\n",
    "    if lambda_i.ndim > 2:\n",
    "        raise ValueError(f\"lambda_i has too many dimensions: {lambda_i.ndim}\")\n",
    "    lambda_i = lambda_i.flatten()  # Ensure it's 1-dimensional\n",
    "    Lambda = np.diag(lambda_i)  # Diagonal matrix from lambda_i\n",
    "    \n",
    "    # Step 5: Construct the full block matrix system\n",
    "    LHS = np.block([\n",
    "        [H_dd, B, C],  # First row block\n",
    "        [Lambda, D, np.zeros((n, 1))],  # Second row block\n",
    "        [M, np.zeros((1, n)), np.array([[np.sum(c * d_i) - Q]])]  # Third row block\n",
    "    ])\n",
    "    \n",
    "    # Step 6: Construct the RHS vector\n",
    "    v = np.array([-alpha * r[i] * (r[i] * d_i[i])**(-alpha - 1) for i in range(n)])\n",
    "    RHS = np.hstack([v, np.zeros(n), np.zeros(1)])\n",
    "    \n",
    "    # Step 7: Solve the matrix system\n",
    "    if is_invertible(LHS):\n",
    "        solution = solve(LHS, RHS)\n",
    "        d_r_derivatives = solution[:n]\n",
    "        return d_r_derivatives\n",
    "    else:\n",
    "        return {\"error\": \"LHS matrix is not invertible. Check problem formulation or input data.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optModelMatrix:\n",
    "    \"\"\"\n",
    "    This is a class for optimization models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r, c, Q, alpha=0.5):\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data = num_data\n",
    "        self.num_items = num_items\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'optModel ' + self.__class__.__name__\n",
    "\n",
    "    def setObj(self, r, c):\n",
    "\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "\n",
    "    def solve(self, closed=False):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        self.d = cp.Variable(self.num_items)\n",
    "        self.setObj(self.r, self.c)\n",
    "        self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "        opt_sol = self.d.value\n",
    "        opt_val = self.problem.value\n",
    "\n",
    "\n",
    "        # Dual values for the constraints\n",
    "        lambdas = self.constraints[0].dual_value\n",
    "        mus = self.constraints[1].dual_value\n",
    "    \n",
    "        return opt_sol, opt_val, lambdas, mus\n",
    "    \n",
    "class optDatasetMatrix(Dataset):\n",
    "    \"\"\"\n",
    "    This class is Torch Dataset class for optimization problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, costs, r, Q, alpha=0.5, closed=False):\n",
    "        \"\"\"\n",
    "        A method to create a optDataset from optModel\n",
    "\n",
    "        Args:\n",
    "            model (optModel): optimization model\n",
    "            features (np.ndarray): features\n",
    "            c (np.ndarray): c of objective function\n",
    "            r (np.ndarray): r of objective function\n",
    "            Q (float): budget\n",
    "            alpha (float): alpha of objective function\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        \"\"\"\n",
    "        self.feats = features\n",
    "        self.costs = costs\n",
    "        self.r = r\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.closed = closed\n",
    "        # Now store the dual values\n",
    "        self.sols, self.objs, self.lambdas, self.mus = self._getSols()\n",
    "\n",
    "    def _getSols(self):\n",
    "        \"\"\"\n",
    "        A method to get the solutions and dual values of the optimization problem\n",
    "        \"\"\"\n",
    "        opt_sols = []\n",
    "        opt_objs = []\n",
    "        dual_lambdas = []\n",
    "        dual_mus = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.costs))):\n",
    "            sol, obj, lambdas, mus = self._solve(self.r[i], self.costs[i])\n",
    "            opt_sols.append(sol)\n",
    "            opt_objs.append([obj])\n",
    "            dual_lambdas.append(lambdas)\n",
    "            dual_mus.append(mus)\n",
    "            \n",
    "        return np.array(opt_sols), np.array(opt_objs), np.array(dual_lambdas), np.array(dual_mus)\n",
    "\n",
    "\n",
    "    def  _solve(self, r, c):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem to get oan optimal solution with given r and c\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): r of objective function\n",
    "            c (np.ndarray): c of objective function\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (np.ndarray), objective value (float)\n",
    "        \"\"\"\n",
    "        self.model = optModelMatrix(r, c, self.Q, self.alpha)\n",
    "        if self.closed:\n",
    "            return self.model.solveC()\n",
    "        else:\n",
    "            return self.model.solve()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        A method to get data size\n",
    "\n",
    "        Returns:\n",
    "            int: the number of optimization problems\n",
    "        \"\"\"\n",
    "        return len(self.costs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.FloatTensor(self.feats[index]),  # x \n",
    "            torch.FloatTensor(self.costs[index]),  # c\n",
    "            torch.FloatTensor(self.r[index]),      # r \n",
    "            torch.FloatTensor(self.sols[index]),   # optimal solution\n",
    "            torch.FloatTensor(self.objs[index]),   # objective value\n",
    "            torch.FloatTensor(self.lambdas[index]),  # dual value (lambdas)\n",
    "            torch.FloatTensor([self.mus[index]])     # dual value (mus)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 1000\n",
    "num_features = 20\n",
    "num_items = 5\n",
    "x, r, c, Q = genData(num_data, num_features, num_items)\n",
    "# create optmodel instance\n",
    "optmodel = optModelMatrix(r, c, Q, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:02<00:00, 297.89it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 298.93it/s]\n"
     ]
    }
   ],
   "source": [
    "optmodel = optModelMatrix(r, c, Q, alpha=0.5)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test, r_train, r_test = train_test_split(x, c, r, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataset_train = optDatasetMatrix(x_train, c_train, r_train, Q, alpha=0.5, closed=False)\n",
    "dataset_test = optDatasetMatrix(x_test, c_test, r_test, Q, alpha=0.5, closed=False)\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
