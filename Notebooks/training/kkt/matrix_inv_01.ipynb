{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0xac4e9b70>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'E:\\\\User\\\\Stevens\\\\Spring 2024\\\\PTO - Fairness\\\\myGit\\\\myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    # Split the population into group A (1/4) and group B (3/4)\n",
    "    group_A_size = n // 4\n",
    "    group_B_size = n - group_A_size\n",
    "    \n",
    "    # Generate x with a bias for group A\n",
    "    x = np.zeros((n, m, p))\n",
    "    x[:group_A_size] = rnd.normal(0.5, 1, (group_A_size, m, p))  # Slightly higher mean for group A\n",
    "    x[group_A_size:] = rnd.normal(0, 1, (group_B_size, m, p))   # Standard distribution for group B\n",
    "    \n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            \n",
    "            # Introduce bias for c: Group A has slightly higher values, Group B slightly lower\n",
    "            if i < group_A_size:\n",
    "                values *= 1.1  # Increase c for Group A\n",
    "            else:\n",
    "                values *= 0.9  # Decrease c for Group B\n",
    "            \n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    r = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            # Generate r using normal distribution, then clip to [0, 1] range\n",
    "            r[i, j] = np.clip(rnd.normal(0.5, 0.2), 0, 1)\n",
    "            \n",
    "            # Introduce bias for r: Group A has slightly lower values, Group B slightly higher\n",
    "            if i < group_A_size:\n",
    "                r[i, j] -= 0.1  # Decrease r for Group A\n",
    "            else:\n",
    "                r[i, j] += 0.1  # Increase r for Group B\n",
    "            \n",
    "            # Clip again to ensure values remain in [0, 1] range\n",
    "            r[i, j] = np.clip(r[i, j], 0, 1)\n",
    "\n",
    "    return x, r, c, Q\n",
    "\n",
    "\n",
    "class optModel:\n",
    "    \"\"\"\n",
    "    This is a class for optimization models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r, c, Q, alpha=0.5):\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data = num_data\n",
    "        self.num_items = num_items\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'optModel ' + self.__class__.__name__\n",
    "    \n",
    "    def setObj2(self, a, r, b, c, Q, epsilon=0.01, alpha=0.5):\n",
    "        if alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(a * r + b * self.d + epsilon))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(a * r + b * self.d + epsilon, 1 - alpha)) / (1 - alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            cp.sum(cp.multiply(c, self.d)) <= Q\n",
    "        ]\n",
    "        \n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "        self.a = a\n",
    "        self.r = r\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    def setObj(self, r, c):\n",
    "\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "\n",
    "    def solve(self, closed=False):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        self.d = cp.Variable(self.num_items)\n",
    "        self.setObj(self.r, self.c)\n",
    "        self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "        opt_sol = self.d.value\n",
    "        opt_val = self.problem.value\n",
    "\n",
    "\n",
    "        # Dual values for the constraints\n",
    "        lambdas = self.constraints[0].dual_value\n",
    "        mus = self.constraints[1].dual_value\n",
    "    \n",
    "        return opt_sol, opt_val, lambdas, mus\n",
    "\n",
    "\n",
    "    def solveC(self):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem in closed form for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        r = self.r\n",
    "        c = self.c\n",
    "        if self.alpha == 1:\n",
    "            raise ValueError(\"Work in progress\")\n",
    "        c = c.cpu().numpy() if isinstance(c, torch.Tensor) else c\n",
    "        r = r.cpu().numpy() if isinstance(r, torch.Tensor) else r\n",
    "        S = np.sum(c ** (1 - 1 / self.alpha) * r ** (-1 + 1 / self.alpha))\n",
    "        opt_sol_c = (c ** (-1 / self.alpha) * r ** (-1 + 1 / self.alpha) * self.Q) / S\n",
    "        opt_val_c = np.sum((r * opt_sol_c) ** (1 - self.alpha)) / (1 - self.alpha)\n",
    "\n",
    "        return opt_sol_c, opt_val_c\n",
    "    \n",
    "    def solveC2(self):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem in closed form using the given formula.\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if self.alpha == 1:\n",
    "            raise ValueError(\"Work in progress\")\n",
    "\n",
    "        a = self.a\n",
    "        r = self.r\n",
    "        b = self.b\n",
    "        c = self.c\n",
    "        epsilon = self.epsilon\n",
    "        Q = self.Q\n",
    "        alpha = self.alpha\n",
    "\n",
    "        b_inverse_alpha = b ** (-1 / alpha)\n",
    "        c_over_b = c / b\n",
    "        ar_plus_epsilon = a * r + epsilon\n",
    "\n",
    "        S1 = np.sum(c_over_b * ar_plus_epsilon)\n",
    "        S2 = np.sum(c_over_b ** (1 - 1 / alpha))\n",
    "\n",
    "        d_star = b_inverse_alpha * (Q + S1) / S2 - ar_plus_epsilon * b_inverse_alpha / b\n",
    "\n",
    "        opt_val_c2 = np.sum((a * r + b * d_star + epsilon) ** (1 - alpha)) / (1 - alpha)\n",
    "\n",
    "        return d_star, opt_val_c2\n",
    "\n",
    "\n",
    "    \n",
    "class optDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is Torch Dataset class for optimization problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, costs, r, Q, alpha=0.5, closed=False):\n",
    "        \"\"\"\n",
    "        A method to create a optDataset from optModel\n",
    "\n",
    "        Args:\n",
    "            model (optModel): optimization model\n",
    "            features (np.ndarray): features\n",
    "            c (np.ndarray): c of objective function\n",
    "            r (np.ndarray): r of objective function\n",
    "            Q (float): budget\n",
    "            alpha (float): alpha of objective function\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        \"\"\"\n",
    "        self.feats = features\n",
    "        self.costs = costs\n",
    "        self.r = r\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.closed = closed\n",
    "        # Now store the dual values\n",
    "        self.sols, self.objs, self.lambdas, self.mus = self._getSols()\n",
    "\n",
    "    def _getSols(self):\n",
    "        \"\"\"\n",
    "        A method to get the solutions and dual values of the optimization problem\n",
    "        \"\"\"\n",
    "        opt_sols = []\n",
    "        opt_objs = []\n",
    "        dual_lambdas = []\n",
    "        dual_mus = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.costs))):\n",
    "            sol, obj, lambdas, mus = self._solve(self.r[i], self.costs[i])\n",
    "            opt_sols.append(sol)\n",
    "            opt_objs.append([obj])\n",
    "            dual_lambdas.append(lambdas)\n",
    "            dual_mus.append(mus)\n",
    "            \n",
    "        return np.array(opt_sols), np.array(opt_objs), np.array(dual_lambdas), np.array(dual_mus)\n",
    "\n",
    "\n",
    "    def  _solve(self, r, c):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem to get oan optimal solution with given r and c\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): r of objective function\n",
    "            c (np.ndarray): c of objective function\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (np.ndarray), objective value (float)\n",
    "        \"\"\"\n",
    "        self.model = optModel(r, c, self.Q, self.alpha)\n",
    "        if self.closed:\n",
    "            return self.model.solveC()\n",
    "        else:\n",
    "            return self.model.solve()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        A method to get data size\n",
    "\n",
    "        Returns:\n",
    "            int: the number of optimization problems\n",
    "        \"\"\"\n",
    "        return len(self.costs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.FloatTensor(self.feats[index]),  # x \n",
    "            torch.FloatTensor(self.costs[index]),  # c\n",
    "            torch.FloatTensor(self.r[index]),      # r \n",
    "            torch.FloatTensor(self.sols[index]),   # optimal solution\n",
    "            torch.FloatTensor(self.objs[index]),   # objective value\n",
    "            torch.FloatTensor(self.lambdas[index]),  # dual value (lambdas)\n",
    "            torch.FloatTensor([self.mus[index]])     # dual value (mus)\n",
    "        )\n",
    "\n",
    "    \n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, num_items, num_features):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_features, 1) for _ in range(num_items)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_items):\n",
    "            outputs.append(torch.sigmoid(self.linears[i](x[:, i, :])))\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "\n",
    "def regret(predmodel, optmodel, dataloader):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "    # load data\n",
    "    for data in dataloader:\n",
    "        x, c, r, d, z  = data\n",
    "        # cuda\n",
    "        if next(predmodel.parameters()).is_cuda:\n",
    "            x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            rp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "        # solve\n",
    "        for j in range(rp.shape[0]):\n",
    "            # accumulate loss\n",
    "            loss += calRegret(optModel, c[j].to(\"cpu\").detach().numpy(), rp[j], r[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "        optsum += abs(z).sum().item()\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return loss / (optsum + 1e-7)\n",
    "\n",
    "def objValue(d, r, alpha=0.5):\n",
    "    \"\"\"\n",
    "    A function to calculate objective value\n",
    "    \"\"\"\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(np.multiply(r, d)))\n",
    "    else:\n",
    "        return np.sum(np.power(np.multiply(r, d), 1 - alpha)) / (1 - alpha)\n",
    "\n",
    "\n",
    "\n",
    "def calRegret(optmodel, cost, pred_r, true_r, true_obj):\n",
    "    \"\"\"\n",
    "    A function to calculate normalized true regret for a batch\n",
    "\n",
    "    Args:\n",
    "        optmodel (optModel): optimization model\n",
    "        pred_cost (torch.tensor): predicted costs\n",
    "        true_cost (torch.tensor): true costs\n",
    "        true_obj (torch.tensor): true optimal objective values\n",
    "\n",
    "    Returns:predmodel\n",
    "        float: true regret losses\n",
    "    \"\"\"\n",
    "    # opt sol for pred cost\n",
    "    model = optmodel(pred_r, cost, Q, alpha=0.5)\n",
    "    sol, _ = model.solve()\n",
    "    # obj with true cost\n",
    "    obj = objValue(sol, true_r, alpha=0.5)\n",
    "    # loss\n",
    "    loss = true_obj - obj\n",
    "    return loss\n",
    "\n",
    "# Define the visualization function\n",
    "def visLearningCurve(loss_log, loss_log_regret):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "    ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve on Training Set\", fontsize=16)\n",
    "\n",
    "    ax2.plot(loss_log_regret, color=\"royalblue\", ls=\"--\", alpha=0.7, lw=2)\n",
    "    ax2.set_xticks(range(0, len(loss_log_regret), 2))\n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_ylim(0, 0.10)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve on Test Set\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 275.52it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 307.49it/s]\n"
     ]
    }
   ],
   "source": [
    "num_data = 10\n",
    "num_features = 10\n",
    "num_items = 5\n",
    "\n",
    "x, r, c, Q = genData(num_data, num_features, num_items)\n",
    "optmodel = optModel(r, c, Q, alpha=0.5)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test, r_train, r_test = train_test_split(x, c, r, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataset_train = optDataset(x_train, c_train, r_train, Q, alpha=0.5, closed=False)\n",
    "dataset_test = optDataset(x_test, c_test, r_test, Q, alpha=0.5, closed=False)\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.2609,  0.9179,  2.1222,  1.0325, -1.5194, -0.4842,  1.2669, -0.7077,\n",
       "           0.4438,  0.7746],\n",
       "         [-0.9269, -0.0595, -3.2413, -1.0244, -0.2526, -1.2478,  1.6324, -1.4301,\n",
       "          -0.4400,  0.1307],\n",
       "         [ 1.4413, -1.4359,  1.1632,  0.0102, -0.9815,  0.4621,  0.1991, -0.6002,\n",
       "           0.0698, -0.3853],\n",
       "         [ 0.1135,  0.6621,  1.5860, -1.2378,  2.1330, -1.9521, -0.1518,  0.5883,\n",
       "           0.2810, -0.6227],\n",
       "         [-0.2081, -0.4930, -0.5894,  0.8496,  0.3570, -0.6929,  0.8996,  0.3073,\n",
       "           0.8129,  0.6296]]),\n",
       " tensor([5., 4., 8., 6., 7.]),\n",
       " tensor([0.8992, 0.7309, 0.5889, 0.6560, 0.3749]),\n",
       " tensor([6.0042, 7.6252, 1.5359, 3.0418, 1.2772]),\n",
       " tensor([15.4798]),\n",
       " tensor([1.3316e-10, 8.6852e-11, 7.0228e-10, 2.6338e-10, 8.6131e-10]),\n",
       " tensor([0.0774]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve\n",
    "\n",
    "# Helper function to check matrix invertibility\n",
    "def is_invertible(matrix):\n",
    "    return np.linalg.cond(matrix) < 1 / np.finfo(matrix.dtype).eps\n",
    "\n",
    "# Function to form the matrix system and solve it\n",
    "def solve_rd_matrix_system(r, c, d_i, mu, lambda_i, Q, alpha = 0.5):\n",
    "    n = len(r)\n",
    "    \n",
    "    # Step 1: Construct Hessian of the objective function (H_dd)\n",
    "    A_elements = [alpha * r[i]**2 * (r[i] * d_i[i])**(-alpha - 1) for i in range(n)]\n",
    "    H_dd = np.diag(A_elements)\n",
    "    \n",
    "    # Step 2: Construct Jacobians of inequality constraints (g(d;r)) and equality constraints (h(d;r))\n",
    "    # Jacobian of g(d;r) = d >= 0 is -I\n",
    "    B = -np.eye(n)\n",
    "    \n",
    "    # Jacobian of h(d;r) = sum(c * d) - Q is the vector of c_i's\n",
    "    C = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        C[i, 0] = c[i]\n",
    "    \n",
    "    # Step 3: Construct D and M matrices\n",
    "    D = np.diag(d_i)  # Diagonal matrix with d_i's\n",
    "    M = np.zeros((1, n))\n",
    "    M[0, :] = mu * c  # Row vector of mu * c\n",
    "    \n",
    "    # Step 4: Complementary slackness matrix with lambda_i\n",
    "    Lambda = np.diag(lambda_i)\n",
    "    \n",
    "    # Step 5: Construct the full block matrix system\n",
    "    LHS = np.block([\n",
    "        [H_dd, B, C],  # First row block\n",
    "        [Lambda, D, np.zeros((n, 1))],  # Second row block\n",
    "        [M, np.zeros((1, n)), np.array([[np.sum(c * d_i) - Q]])]  # Third row block\n",
    "    ])\n",
    "    \n",
    "    # Step 6: Construct the RHS vector\n",
    "    # RHS vector contains v = -α r (r d)^{-α-1}, 0 for other conditions\n",
    "    v = np.array([-alpha * r[i] * (r[i] * d_i[i])**(-alpha - 1) for i in range(n)])\n",
    "    RHS = np.hstack([v, np.zeros(n), np.zeros(1)])\n",
    "    \n",
    "    # Step 7: Solve the matrix system\n",
    "    if is_invertible(LHS):\n",
    "        solution = solve(LHS, RHS)\n",
    "        d_r_derivatives = solution[:n]\n",
    "        lambda_r_derivatives = solution[n:2*n]\n",
    "        mu_r_derivative = solution[2*n:]\n",
    "        \n",
    "        return d_r_derivatives\n",
    "\n",
    "\n",
    "# Provided data\n",
    "c = np.array([5., 3., 4., 8., 9.])  # c values\n",
    "r = np.array([0.9055, 1.0000, 1.0000, 1.0000, 0.9991])  # r values\n",
    "d_i = np.array([3.6203, 11.1061, 6.2473, 1.5618, 1.2329])  # Optimal d values\n",
    "z = np.array([20.0043])  # Objective value (used for reference, not in matrix calculation)\n",
    "dual_lambda = np.array([6.16273581e-10, 1.82440296e-10, 4.25497324e-10, 1.70830862e-09, 2.18849516e-09])  # Dual lambda values\n",
    "dual_mu = 0.10002232440085013  # Dual mu value\n",
    "Q = 100  # Equality constraint\n",
    "alpha = 0.5  # Alpha value for fairness objective\n",
    "\n",
    "# Solve the system with the given data\n",
    "solution = solve_rd_matrix_system(r, c, Q, alpha, d_i, dual_mu, dual_lambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivatives of d_i with respect to r_i: [3.27446649 7.40576132 4.68611142 1.36666686 1.09709044]\n",
      "[-0.03529868  2.27962759  0.84484312 -0.53880029 -0.63682593]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def partial_derivative_d_i_wrt_r_i(c, r, Q, alpha):\n",
    "    A = c**(-1/alpha) * r**(-1 + 1/alpha)\n",
    "    S = np.sum(c**(1 - 1/alpha) * r**(-1 + 1/alpha))\n",
    "    \n",
    "    d_dr = []\n",
    "    \n",
    "    for i in range(len(r)):\n",
    "        A_i = c[i]**(-1/alpha) * r[i]**(-1 + 1/alpha)\n",
    "        partial_A_i_wrt_r_i = c[i]**(-1/alpha) * (-1 + 1/alpha) * r[i]**(-2 + 1/alpha)\n",
    "        partial_S_wrt_r_i = c[i]**(1 - 1/alpha) * (-1 + 1/alpha) * r[i]**(-2 + 1/alpha)\n",
    "        \n",
    "        numerator = Q * (partial_A_i_wrt_r_i * S - A_i * partial_S_wrt_r_i)\n",
    "        denominator = S**2\n",
    "        \n",
    "        derivative = numerator / denominator\n",
    "        \n",
    "        d_dr.append(derivative)\n",
    "\n",
    "    return np.array(d_dr)\n",
    "\n",
    "derivatives = partial_derivative_d_i_wrt_r_i(c, r, Q, alpha)\n",
    "\n",
    "print(\"Partial derivatives of d_i with respect to r_i:\", derivatives)\n",
    "\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivatives of d_i with respect to r_i: [2.08573851 3.37613329 1.48244495 1.73314022 3.71249268]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([8., 6., 10., 9., 6.]) \n",
    "r = np.array([1.0000, 1.0000, 0.7353, 0.9085, 0.7466])  \n",
    "d_i = np.array([2.6457, 4.7035, 1.2451, 1.8992, 3.5117]) \n",
    "z = np.array([15.3699])  # Objective value for reference\n",
    "dual_lambda = np.array([3.51481403e-10, 2.02084120e-10, 7.82048660e-10, 5.05606257e-10, 2.61115052e-10])  # Dual lambda values\n",
    "dual_mu = 0.07684988998398859  # Dual mu value\n",
    "Q = 100\n",
    "alpha = 0.5  \n",
    "\n",
    "# Calculate the partial derivatives\n",
    "derivatives = partial_derivative_d_i_wrt_r_i(c, r, Q, alpha)\n",
    "\n",
    "# Print the results\n",
    "print(\"Partial derivatives of d_i with respect to r_i:\", derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(solution[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDerivatives of d with respect to r:\u001b[39m\u001b[38;5;124m\"\u001b[39m, solution[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_r_derivatives\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDerivatives of lambda with respect to r:\u001b[39m\u001b[38;5;124m\"\u001b[39m, solution[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlambda_r_derivatives\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDerivative of mu with respect to r:\u001b[39m\u001b[38;5;124m\"\u001b[39m, solution[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmu_r_derivative\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Solve the system with the given data\n",
    "solution = solve_rd_matrix_system(r, c, Q, alpha, d_i, dual_mu, dual_lambda)\n",
    "\n",
    "# Output the results\n",
    "if \"error\" in solution:\n",
    "    print(solution[\"error\"])\n",
    "else:\n",
    "    print(\"Derivatives of d with respect to r:\", solution[\"d_r_derivatives\"])\n",
    "    print(\"Derivatives of lambda with respect to r:\", solution[\"lambda_r_derivatives\"])\n",
    "    print(\"Derivative of mu with respect to r:\", solution[\"mu_r_derivative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
