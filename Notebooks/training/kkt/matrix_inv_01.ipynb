{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d568d30>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'E:\\\\User\\\\Stevens\\\\Spring 2024\\\\PTO - Fairness\\\\myGit\\\\myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    # Split the population into group A (1/4) and group B (3/4)\n",
    "    group_A_size = n // 4\n",
    "    group_B_size = n - group_A_size\n",
    "    \n",
    "    # Generate x with a bias for group A\n",
    "    x = np.zeros((n, m, p))\n",
    "    x[:group_A_size] = rnd.normal(0.5, 1, (group_A_size, m, p))  # Slightly higher mean for group A\n",
    "    x[group_A_size:] = rnd.normal(0, 1, (group_B_size, m, p))   # Standard distribution for group B\n",
    "    \n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            \n",
    "            # Introduce bias for c: Group A has slightly higher values, Group B slightly lower\n",
    "            if i < group_A_size:\n",
    "                values *= 1.1  # Increase c for Group A\n",
    "            else:\n",
    "                values *= 0.9  # Decrease c for Group B\n",
    "            \n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    \n",
    "    w = rnd.normal(0, 1, (m, p))\n",
    "    \n",
    "    # Generate b with a slight bias for group A\n",
    "    b = np.zeros((n, m))\n",
    "    b[:group_A_size] = rnd.normal(0.5, 1, (group_A_size, m))  # Slightly higher bias for group A\n",
    "    b[group_A_size:] = rnd.normal(0, 1, (group_B_size, m))    # Standard distribution for group B\n",
    "    \n",
    "    r = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            r[i, j] = np.dot(w[j], x[i, j])**2 + b[i, j]\n",
    "            \n",
    "            # Introduce bias for r: Group A has slightly lower values, Group B slightly higher\n",
    "            if i < group_A_size:\n",
    "                r[i, j] -= 0.2  # Decrease r for Group A\n",
    "            else:\n",
    "                r[i, j] += 0.2  # Increase r for Group B\n",
    "\n",
    "    r = 1 / (1 + np.exp(-r))  # Sigmoid function to r\n",
    "\n",
    "    return x, r, c, Q\n",
    "\n",
    "\n",
    "class optModel:\n",
    "    \"\"\"\n",
    "    This is a class for optimization models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r, c, Q, alpha=0.5):\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data = num_data\n",
    "        self.num_items = num_items\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'optModel ' + self.__class__.__name__\n",
    "    \n",
    "    def setObj2(self, a, r, b, c, Q, epsilon=0.01, alpha=0.5):\n",
    "        if alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(a * r + b * self.d + epsilon))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(a * r + b * self.d + epsilon, 1 - alpha)) / (1 - alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            cp.sum(cp.multiply(c, self.d)) <= Q\n",
    "        ]\n",
    "        \n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "        self.a = a\n",
    "        self.r = r\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    def setObj(self, r, c):\n",
    "\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "\n",
    "    def solve(self, closed=False):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        self.d = cp.Variable(self.num_items)\n",
    "        self.setObj(self.r, self.c)\n",
    "        self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "        opt_sol = self.d.value\n",
    "        opt_val = self.problem.value\n",
    "\n",
    "        self.duals = (self.constraints[0].dual_value, self.constraints[1].dual_value)\n",
    "        print(self.duals)\n",
    "    \n",
    "\n",
    "        return opt_sol, opt_val\n",
    "\n",
    "\n",
    "    def solveC(self):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem in closed form for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        r = self.r\n",
    "        c = self.c\n",
    "        if self.alpha == 1:\n",
    "            raise ValueError(\"Work in progress\")\n",
    "        c = c.cpu().numpy() if isinstance(c, torch.Tensor) else c\n",
    "        r = r.cpu().numpy() if isinstance(r, torch.Tensor) else r\n",
    "        S = np.sum(c ** (1 - 1 / self.alpha) * r ** (-1 + 1 / self.alpha))\n",
    "        opt_sol_c = (c ** (-1 / self.alpha) * r ** (-1 + 1 / self.alpha) * self.Q) / S\n",
    "        opt_val_c = np.sum((r * opt_sol_c) ** (1 - self.alpha)) / (1 - self.alpha)\n",
    "\n",
    "        return opt_sol_c, opt_val_c\n",
    "    \n",
    "    def solveC2(self):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem in closed form using the given formula.\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if self.alpha == 1:\n",
    "            raise ValueError(\"Work in progress\")\n",
    "\n",
    "        a = self.a\n",
    "        r = self.r\n",
    "        b = self.b\n",
    "        c = self.c\n",
    "        epsilon = self.epsilon\n",
    "        Q = self.Q\n",
    "        alpha = self.alpha\n",
    "\n",
    "        b_inverse_alpha = b ** (-1 / alpha)\n",
    "        c_over_b = c / b\n",
    "        ar_plus_epsilon = a * r + epsilon\n",
    "\n",
    "        S1 = np.sum(c_over_b * ar_plus_epsilon)\n",
    "        S2 = np.sum(c_over_b ** (1 - 1 / alpha))\n",
    "\n",
    "        d_star = b_inverse_alpha * (Q + S1) / S2 - ar_plus_epsilon * b_inverse_alpha / b\n",
    "\n",
    "        opt_val_c2 = np.sum((a * r + b * d_star + epsilon) ** (1 - alpha)) / (1 - alpha)\n",
    "\n",
    "        return d_star, opt_val_c2\n",
    "\n",
    "\n",
    "    \n",
    "class optDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is Torch Dataset class for optimization problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, costs, r, Q, alpha=0.5, closed=False):\n",
    "        \"\"\"\n",
    "        A method to create a optDataset from optModel\n",
    "\n",
    "        Args:\n",
    "            model (optModel): optimization model\n",
    "            features (np.ndarray): features\n",
    "            c (np.ndarray): c of objective function\n",
    "            r (np.ndarray): r of objective function\n",
    "            Q (float): budget\n",
    "            alpha (float): alpha of objective function\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        \"\"\"\n",
    "        self.feats = features\n",
    "        self.costs = costs\n",
    "        self.r = r\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.closed = closed\n",
    "\n",
    "        self.sols, self.objs = self._getSols()\n",
    "\n",
    "    def _getSols(self):\n",
    "        \"\"\"\n",
    "        A method to get the solutions of the optimization problem\n",
    "        \"\"\"\n",
    "        opt_sols = []\n",
    "        opt_objs = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.costs))):\n",
    "            sol, obj = self._solve(self.r[i], self.costs[i])\n",
    "            opt_sols.append(sol)\n",
    "            opt_objs.append([obj])\n",
    "        \n",
    "        return np.array(opt_sols), np.array(opt_objs)\n",
    "\n",
    "    def  _solve(self, r, c):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem to get oan optimal solution with given r and c\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): r of objective function\n",
    "            c (np.ndarray): c of objective function\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (np.ndarray), objective value (float)\n",
    "        \"\"\"\n",
    "        self.model = optModel(r, c, self.Q, self.alpha)\n",
    "        if self.closed:\n",
    "            return self.model.solveC()\n",
    "        else:\n",
    "            return self.model.solve()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        A method to get data size\n",
    "\n",
    "        Returns:\n",
    "            int: the number of optimization problems\n",
    "        \"\"\"\n",
    "        return len(self.costs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        A method to retrieve data\n",
    "\n",
    "        Args:\n",
    "            index (int): data index\n",
    "\n",
    "        Returns:\n",
    "            tuple: data features (torch.tensor), costs (torch.tensor), optimal solutions (torch.tensor) and objective values (torch.tensor)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            torch.FloatTensor(self.feats[index]), # x \n",
    "            torch.FloatTensor(self.costs[index]), # c\n",
    "            torch.FloatTensor(self.r[index]), # r \n",
    "            torch.FloatTensor(self.sols[index]),# optimal solution\n",
    "            torch.FloatTensor(self.objs[index]), # objective value\n",
    "        )\n",
    "    \n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, num_items, num_features):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_features, 1) for _ in range(num_items)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_items):\n",
    "            outputs.append(torch.sigmoid(self.linears[i](x[:, i, :])))\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "\n",
    "def regret(predmodel, optmodel, dataloader):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "    # load data\n",
    "    for data in dataloader:\n",
    "        x, c, r, d, z  = data\n",
    "        # cuda\n",
    "        if next(predmodel.parameters()).is_cuda:\n",
    "            x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            rp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "        # solve\n",
    "        for j in range(rp.shape[0]):\n",
    "            # accumulate loss\n",
    "            loss += calRegret(optModel, c[j].to(\"cpu\").detach().numpy(), rp[j], r[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "        optsum += abs(z).sum().item()\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return loss / (optsum + 1e-7)\n",
    "\n",
    "def objValue(d, r, alpha=0.5):\n",
    "    \"\"\"\n",
    "    A function to calculate objective value\n",
    "    \"\"\"\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(np.multiply(r, d)))\n",
    "    else:\n",
    "        return np.sum(np.power(np.multiply(r, d), 1 - alpha)) / (1 - alpha)\n",
    "\n",
    "\n",
    "\n",
    "def calRegret(optmodel, cost, pred_r, true_r, true_obj):\n",
    "    \"\"\"\n",
    "    A function to calculate normalized true regret for a batch\n",
    "\n",
    "    Args:\n",
    "        optmodel (optModel): optimization model\n",
    "        pred_cost (torch.tensor): predicted costs\n",
    "        true_cost (torch.tensor): true costs\n",
    "        true_obj (torch.tensor): true optimal objective values\n",
    "\n",
    "    Returns:predmodel\n",
    "        float: true regret losses\n",
    "    \"\"\"\n",
    "    # opt sol for pred cost\n",
    "    model = optmodel(pred_r, cost, Q, alpha=0.5)\n",
    "    sol, _ = model.solve()\n",
    "    # obj with true cost\n",
    "    obj = objValue(sol, true_r, alpha=0.5)\n",
    "    # loss\n",
    "    loss = true_obj - obj\n",
    "    return loss\n",
    "\n",
    "# Define the visualization function\n",
    "def visLearningCurve(loss_log, loss_log_regret):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "    ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve on Training Set\", fontsize=16)\n",
    "\n",
    "    ax2.plot(loss_log_regret, color=\"royalblue\", ls=\"--\", alpha=0.7, lw=2)\n",
    "    ax2.set_xticks(range(0, len(loss_log_regret), 2))\n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_ylim(0, 0.10)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve on Test Set\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 232.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([6.16273581e-10, 1.82440296e-10, 4.25497324e-10, 1.70830862e-09,\n",
      "       2.18849516e-09]), 0.10002232440085013)\n",
      "(array([3.51481403e-10, 2.02084120e-10, 7.82048660e-10, 5.05606257e-10,\n",
      "       2.61115052e-10]), 0.07684988998398859)\n",
      "(array([2.65073757e-10, 2.65073757e-10, 2.65074119e-10, 1.10265665e-10,\n",
      "       5.09737679e-11]), 0.10407448692450037)\n",
      "(array([1.78135049e-09, 7.62932576e-10, 3.71706445e-10, 3.69338079e-10,\n",
      "       1.52980804e-10]), 0.08920101361834666)\n",
      "(array([2.47156318e-10, 6.13946410e-10, 4.79342557e-10, 1.03971722e-09,\n",
      "       3.48868792e-10]), 0.08437169902708166)\n",
      "(array([3.07939057e-10, 3.07943197e-10, 3.07943197e-10, 1.77790854e-10,\n",
      "       3.06706300e-10]), 0.10538636679498493)\n",
      "(array([1.32323309e-10, 1.75731757e-10, 5.58460679e-11, 4.17023733e-10,\n",
      "       5.52547008e-10]), 0.10110473118155379)\n",
      "(array([1.49636361e-10, 3.59735824e-10, 5.29986566e-10, 2.26271451e-10,\n",
      "       2.26156671e-10]), 0.0979523635105033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 285.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2.96227935e-10, 3.24025895e-10, 2.96232846e-10, 4.09908575e-10,\n",
      "       1.79817728e-09]), 0.08266302791046345)\n",
      "(array([1.14404549e-10, 6.94155157e-11, 3.22124531e-10, 3.75341009e-10,\n",
      "       1.35215084e-10]), 0.0821402199089975)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_data = 10\n",
    "num_features = 20\n",
    "num_items = 5\n",
    "\n",
    "x, r, c, Q = genData(num_data, num_features, num_items)\n",
    "optmodel = optModel(r, c, Q, alpha=0.5)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test, r_train, r_test = train_test_split(x, c, r, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataset_train = optDataset(x_train, c_train, r_train, Q, alpha=0.5, closed=False)\n",
    "dataset_test = optDataset(x_test, c_test, r_test, Q, alpha=0.5, closed=False)\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 9.9671e-01,  3.6174e-01,  1.1477e+00,  2.0230e+00,  2.6585e-01,\n",
       "           2.6586e-01,  2.0792e+00,  1.2674e+00,  3.0526e-02,  1.0426e+00,\n",
       "           3.6582e-02,  3.4270e-02,  7.4196e-01, -1.4133e+00, -1.2249e+00,\n",
       "          -6.2288e-02, -5.1283e-01,  8.1425e-01, -4.0802e-01, -9.1230e-01],\n",
       "         [ 1.9656e+00,  2.7422e-01,  5.6753e-01, -9.2475e-01, -4.4383e-02,\n",
       "           6.1092e-01, -6.5099e-01,  8.7570e-01, -1.0064e-01,  2.0831e-01,\n",
       "          -1.0171e-01,  2.3523e+00,  4.8650e-01, -5.5771e-01,  1.3225e+00,\n",
       "          -7.2084e-01,  7.0886e-01, -1.4597e+00, -8.2819e-01,  6.9686e-01],\n",
       "         [ 1.2385e+00,  6.7137e-01,  3.8435e-01,  1.9890e-01, -9.7852e-01,\n",
       "          -2.1984e-01,  3.9361e-02,  1.5571e+00,  8.4362e-01, -1.2630e+00,\n",
       "           8.2408e-01,  1.1492e-01, -1.7692e-01,  1.1117e+00,  1.5310e+00,\n",
       "           1.4313e+00, -3.3922e-01,  1.9079e-01,  8.3126e-01,  1.4755e+00],\n",
       "         [ 2.0826e-02,  3.1434e-01, -6.0633e-01, -6.9621e-01,  1.3125e+00,\n",
       "           1.8562e+00,  4.2799e-01,  1.5035e+00,  8.6164e-01, -1.4512e-01,\n",
       "           8.6140e-01,  2.0380e+00,  4.6417e-01,  2.0646e+00, -2.1197e+00,\n",
       "           1.3219e+00,  5.8705e-01,  2.0099e-01,  5.9176e-01, -1.4876e+00],\n",
       "         [ 2.8033e-01,  8.5711e-01,  1.9779e+00, -1.8270e-02, -3.0849e-01,\n",
       "          -1.7570e-03,  1.4154e+00,  8.2875e-01, -2.9760e-02,  1.0133e+00,\n",
       "           5.9708e-01,  1.4686e+00, -2.0205e-01,  1.7234e-01,  1.0789e-01,\n",
       "          -9.6351e-01,  7.9612e-01,  7.6106e-01,  5.0511e-01,  2.6541e-01]]),\n",
       " tensor([ 8.,  6., 10.,  9.,  6.]),\n",
       " tensor([1.0000, 1.0000, 0.7353, 0.9085, 0.7466]),\n",
       " tensor([2.6457, 4.7035, 1.2451, 1.8992, 3.5117]),\n",
       " tensor([15.3699]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset_train[1]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.,  6., 10.,  9.,  6.]) tensor([1.0000, 1.0000, 0.7353, 0.9085, 0.7466]) tensor([2.6457, 4.7035, 1.2451, 1.8992, 3.5117]) tensor([15.3699])\n"
     ]
    }
   ],
   "source": [
    "c = data[1]\n",
    "r = data[2]\n",
    "d = data[3]\n",
    "z = data[4]\n",
    "print(c, r, d, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.16273581e-10 1.82440296e-10 4.25497324e-10 1.70830862e-09\n",
      " 2.18849516e-09] 0.10002232440085013\n"
     ]
    }
   ],
   "source": [
    "list = [6.16273581e-10, 1.82440296e-10, 4.25497324e-10, 1.70830862e-09,\n",
    "       2.18849516e-09]\n",
    "dual_lambda = np.array(list)\n",
    "dual_mu = 0.10002232440085013\n",
    "print(dual_lambda, dual_mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve\n",
    "\n",
    "# Helper function to check matrix invertibility\n",
    "def is_invertible(matrix):\n",
    "    return np.linalg.cond(matrix) < 1 / np.finfo(matrix.dtype).eps\n",
    "\n",
    "# Function to form the matrix system and solve it\n",
    "def solve_rd_matrix_system(r, c, Q, alpha, d_i, mu, lambda_i):\n",
    "    n = len(r)\n",
    "    \n",
    "    # Step 1: Construct Hessian of the objective function (H_dd)\n",
    "    A_elements = [alpha * r[i]**2 * (r[i] * d_i[i])**(-alpha - 1) for i in range(n)]\n",
    "    H_dd = np.diag(A_elements)\n",
    "    \n",
    "    # Step 2: Construct Jacobians of inequality constraints (g(d;r)) and equality constraints (h(d;r))\n",
    "    # Jacobian of g(d;r) = d >= 0 is -I\n",
    "    B = -np.eye(n)\n",
    "    \n",
    "    # Jacobian of h(d;r) = sum(c * d) - Q is the vector of c_i's\n",
    "    C = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        C[i, 0] = c[i]\n",
    "    \n",
    "    # Step 3: Construct D and M matrices\n",
    "    D = np.diag(d_i)  # Diagonal matrix with d_i's\n",
    "    M = np.zeros((1, n))\n",
    "    M[0, :] = mu * c  # Row vector of mu * c\n",
    "    \n",
    "    # Step 4: Complementary slackness matrix with lambda_i\n",
    "    Lambda = np.diag(lambda_i)\n",
    "    \n",
    "    # Step 5: Construct the full block matrix system\n",
    "    LHS = np.block([\n",
    "        [H_dd, B, C],  # First row block\n",
    "        [Lambda, D, np.zeros((n, 1))],  # Second row block\n",
    "        [M, np.zeros((1, n)), np.array([[np.sum(c * d_i) - Q]])]  # Third row block\n",
    "    ])\n",
    "    \n",
    "    # Step 6: Construct the RHS vector\n",
    "    # RHS vector contains v = -α r (r d)^{-α-1}, 0 for other conditions\n",
    "    v = np.array([-alpha * r[i] * (r[i] * d_i[i])**(-alpha - 1) for i in range(n)])\n",
    "    RHS = np.hstack([v, np.zeros(n), np.zeros(1)])\n",
    "    \n",
    "    # Step 7: Solve the matrix system\n",
    "    if is_invertible(LHS):\n",
    "        solution = solve(LHS, RHS)\n",
    "        d_r_derivatives = solution[:n]\n",
    "        lambda_r_derivatives = solution[n:2*n]\n",
    "        mu_r_derivative = solution[2*n:]\n",
    "        \n",
    "        return d_r_derivatives\n",
    "        return {\n",
    "            \"error\": \"LHS matrix is not invertible. Check problem formulation or input data.\"\n",
    "        }\n",
    "\n",
    "# Provided data\n",
    "c = np.array([5., 3., 4., 8., 9.])  # c values\n",
    "r = np.array([0.9055, 1.0000, 1.0000, 1.0000, 0.9991])  # r values\n",
    "d_i = np.array([3.6203, 11.1061, 6.2473, 1.5618, 1.2329])  # Optimal d values\n",
    "z = np.array([20.0043])  # Objective value (used for reference, not in matrix calculation)\n",
    "dual_lambda = np.array([6.16273581e-10, 1.82440296e-10, 4.25497324e-10, 1.70830862e-09, 2.18849516e-09])  # Dual lambda values\n",
    "dual_mu = 0.10002232440085013  # Dual mu value\n",
    "Q = 100  # Equality constraint\n",
    "alpha = 0.5  # Alpha value for fairness objective\n",
    "\n",
    "# Solve the system with the given data\n",
    "solution = solve_rd_matrix_system(r, c, Q, alpha, d_i, dual_mu, dual_lambda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivatives of d_i with respect to r_i: [3.27446649 7.40576132 4.68611142 1.36666686 1.09709044]\n",
      "Derivatives of d with respect to r: [-0.03529868  2.27962759  0.84484312 -0.53880029 -0.63682593]\n",
      "Derivatives of lambda with respect to r: [ 6.00879518e-12 -3.74475227e-11 -5.75414156e-11  5.89343823e-10\n",
      "  1.13041646e-09]\n",
      "Derivative of mu with respect to r: [-0.01476831]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def partial_derivative_d_i_wrt_r_i(c, r, Q, alpha):\n",
    "    A = c**(-1/alpha) * r**(-1 + 1/alpha)\n",
    "    S = np.sum(c**(1 - 1/alpha) * r**(-1 + 1/alpha))\n",
    "    \n",
    "    d_dr = []\n",
    "    \n",
    "    for i in range(len(r)):\n",
    "        A_i = c[i]**(-1/alpha) * r[i]**(-1 + 1/alpha)\n",
    "        partial_A_i_wrt_r_i = c[i]**(-1/alpha) * (-1 + 1/alpha) * r[i]**(-2 + 1/alpha)\n",
    "        partial_S_wrt_r_i = c[i]**(1 - 1/alpha) * (-1 + 1/alpha) * r[i]**(-2 + 1/alpha)\n",
    "        \n",
    "        numerator = Q * (partial_A_i_wrt_r_i * S - A_i * partial_S_wrt_r_i)\n",
    "        denominator = S**2\n",
    "        \n",
    "        derivative = numerator / denominator\n",
    "        \n",
    "        d_dr.append(derivative)\n",
    "\n",
    "    return np.array(d_dr)\n",
    "\n",
    "derivatives = partial_derivative_d_i_wrt_r_i(c, r, Q, alpha)\n",
    "\n",
    "print(\"Partial derivatives of d_i with respect to r_i:\", derivatives)\n",
    "\n",
    "if \"error\" in solution:\n",
    "    print(solution[\"error\"])\n",
    "else:\n",
    "    print(\"Derivatives of d with respect to r:\", solution[\"d_r_derivatives\"])\n",
    "    print(\"Derivatives of lambda with respect to r:\", solution[\"lambda_r_derivatives\"])\n",
    "    print(\"Derivative of mu with respect to r:\", solution[\"mu_r_derivative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivatives of d_i with respect to r_i: [2.08573851 3.37613329 1.48244495 1.73314022 3.71249268]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([8., 6., 10., 9., 6.]) \n",
    "r = np.array([1.0000, 1.0000, 0.7353, 0.9085, 0.7466])  \n",
    "d_i = np.array([2.6457, 4.7035, 1.2451, 1.8992, 3.5117]) \n",
    "z = np.array([15.3699])  # Objective value for reference\n",
    "dual_lambda = np.array([3.51481403e-10, 2.02084120e-10, 7.82048660e-10, 5.05606257e-10, 2.61115052e-10])  # Dual lambda values\n",
    "dual_mu = 0.07684988998398859  # Dual mu value\n",
    "Q = 100\n",
    "alpha = 0.5  \n",
    "\n",
    "# Calculate the partial derivatives\n",
    "derivatives = partial_derivative_d_i_wrt_r_i(c, r, Q, alpha)\n",
    "\n",
    "# Print the results\n",
    "print(\"Partial derivatives of d_i with respect to r_i:\", derivatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivatives of d with respect to r: [ 0.204911    1.14208677 -0.79293024 -0.2357671   0.25992238]\n",
      "Derivatives of lambda with respect to r: [-2.72224387e-11 -4.90693314e-11  4.98040345e-10  6.27660701e-11\n",
      " -1.93267211e-11]\n",
      "Derivative of mu with respect to r: [-0.01749944]\n"
     ]
    }
   ],
   "source": [
    "# Solve the system with the given data\n",
    "solution = solve_rd_matrix_system(r, c, Q, alpha, d_i, dual_mu, dual_lambda)\n",
    "\n",
    "# Output the results\n",
    "if \"error\" in solution:\n",
    "    print(solution[\"error\"])\n",
    "else:\n",
    "    print(\"Derivatives of d with respect to r:\", solution[\"d_r_derivatives\"])\n",
    "    print(\"Derivatives of lambda with respect to r:\", solution[\"lambda_r_derivatives\"])\n",
    "    print(\"Derivative of mu with respect to r:\", solution[\"mu_r_derivative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
