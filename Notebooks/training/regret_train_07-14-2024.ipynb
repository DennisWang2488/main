{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d368c10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'E:\\\\User\\\\Stevens\\\\Spring 2024\\\\PTO - Fairness\\\\myGit\\\\myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_data = 100\n",
    "num_features = 5\n",
    "num_items = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    x = rnd.normal(0, 1, (n, m, p))\n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    \n",
    "    w = rnd.normal(0, 1, (m, p))\n",
    "    b = rnd.normal(0, 1, (n, m))\n",
    "    r = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            r[i, j] = np.dot(w[j], x[i, j]) + b[i, j]\n",
    "\n",
    "    r = 1 / (1 + np.exp(-r))\n",
    "\n",
    "    return x, r, c, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optModel:\n",
    "    \"\"\"\n",
    "    This is a class for optimization models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r, c, Q, alpha):\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data = num_data\n",
    "        self.num_items = num_items\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'optModel ' + self.__class__.__name__\n",
    "        \n",
    "\n",
    "    def setObj(self, r, c):\n",
    "\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "    \n",
    "    def solve(self, closed=False):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        self.d = cp.Variable(self.num_items)\n",
    "        self.setObj(self.r, self.c)\n",
    "        self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "        opt_sol = self.d.value\n",
    "        opt_val = self.problem.value\n",
    "\n",
    "        return opt_sol, opt_val\n",
    "\n",
    "\n",
    "    def solveC(self):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem in closed form for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        r = self.r\n",
    "        c = self.c\n",
    "        if self.alpha == 1:\n",
    "            raise ValueError(\"Work in progress\")\n",
    "        \n",
    "        S = np.sum(c ** (1 - 1 / self.alpha) * r ** (-1 + 1 / self.alpha))\n",
    "        opt_sol_c = (c ** (-1 / self.alpha) * r ** (-1 + 1 / self.alpha) * self.Q) / S\n",
    "        opt_val_c = np.sum((r * opt_sol_c) ** (1 - self.alpha)) / (1 - self.alpha)\n",
    "\n",
    "        return opt_sol_c, opt_val_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is Torch Dataset class for optimization problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, costs, r, Q, alpha=0.5, closed=False):\n",
    "        \"\"\"\n",
    "        A method to create a optDataset from optModel\n",
    "\n",
    "        Args:\n",
    "            model (optModel): optimization model\n",
    "            features (np.ndarray): features\n",
    "            c (np.ndarray): c of objective function\n",
    "            r (np.ndarray): r of objective function\n",
    "            Q (float): budget\n",
    "            alpha (float): alpha of objective function\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        \"\"\"\n",
    "        self.feats = features\n",
    "        self.costs = costs\n",
    "        self.r = r\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.closed = closed\n",
    "\n",
    "        self.sols, self.objs = self._getSols()\n",
    "\n",
    "    def _getSols(self):\n",
    "        \"\"\"\n",
    "        A method to get the solutions of the optimization problem\n",
    "        \"\"\"\n",
    "        opt_sols = []\n",
    "        opt_objs = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.costs))):\n",
    "            sol, obj = self._solve(self.r[i], self.costs[i])\n",
    "            opt_sols.append(sol)\n",
    "            opt_objs.append([obj])\n",
    "        \n",
    "        return np.array(opt_sols), np.array(opt_objs)\n",
    "\n",
    "    def  _solve(self, r, c):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem to get oan optimal solution with given r and c\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): r of objective function\n",
    "            c (np.ndarray): c of objective function\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (np.ndarray), objective value (float)\n",
    "        \"\"\"\n",
    "        self.model = optModel(r, c, self.Q, self.alpha)\n",
    "        if self.closed:\n",
    "            return self.model.solveC()\n",
    "        else:\n",
    "            return self.model.solve()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        A method to get data size\n",
    "\n",
    "        Returns:\n",
    "            int: the number of optimization problems\n",
    "        \"\"\"\n",
    "        return len(self.costs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        A method to retrieve data\n",
    "\n",
    "        Args:\n",
    "            index (int): data index\n",
    "\n",
    "        Returns:\n",
    "            tuple: data features (torch.tensor), costs (torch.tensor), optimal solutions (torch.tensor) and objective values (torch.tensor)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            torch.FloatTensor(self.feats[index]), # x \n",
    "            torch.FloatTensor(self.costs[index]), # c\n",
    "            torch.FloatTensor(self.r[index]), # r \n",
    "            torch.FloatTensor(self.sols[index]),# optimal solution\n",
    "            torch.FloatTensor(self.objs[index]), # objective value\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, r, c, Q = genData(num_data, num_features, num_items)\n",
    "optmodel = optModel(r, c, Q, alpha=0.5)\n",
    "# sols = []\n",
    "# objs = []\n",
    "# print(\"Solving the optimization problem...\")\n",
    "# time.sleep(1)\n",
    "\n",
    "# for i in tqdm(range(num_data)):\n",
    "#     sol, obj = optmodel.solve(r[i], c[i])\n",
    "\n",
    "#     sols.append(sol)\n",
    "#     objs.append([obj])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, c_train, c_test, r_train, r_test = train_test_split(x, c, r, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 293.02it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 293.41it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = optDataset(x_train, c_train, r_train, Q, alpha=0.5, closed=False)\n",
    "dataset_test = optDataset(x_test, c_test, r_test, Q, alpha=0.5, closed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=5, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearRegressionOne(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearRegressionOne, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, num_items, num_features):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_features, 1) for _ in range(num_items)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_items):\n",
    "            outputs.append(torch.sigmoid(self.linears[i](x[:, i, :])))\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "model = LinearRegressionModel(num_items, num_features)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.900128364562988\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "for data in loader_train:\n",
    "    x, c, r, d, z  = data\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "        \n",
    "    break\n",
    "print(z[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(predmodel, optmodel, dataloader):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "    # load data\n",
    "    for data in dataloader:\n",
    "        x, c, r, d, z  = data\n",
    "        # cuda\n",
    "        if next(predmodel.parameters()).is_cuda:\n",
    "            x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            rp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "        # solve\n",
    "        for j in range(rp.shape[0]):\n",
    "            # accumulate loss\n",
    "            loss += calRegret(optModel, c[j].to(\"cpu\").detach().numpy(), rp[j], r[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "        optsum += abs(z).sum().item()\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return loss / (optsum + 1e-7)\n",
    "\n",
    "def objValue(d, r, alpha):\n",
    "    \"\"\"\n",
    "    A function to calculate objective value\n",
    "    \"\"\"\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(np.multiply(r, d)))\n",
    "    else:\n",
    "        return np.sum(np.power(np.multiply(r, d), 1 - alpha)) / (1 - alpha)\n",
    "\n",
    "\n",
    "\n",
    "def calRegret(optmodel, cost, pred_r, true_r, true_obj):\n",
    "    \"\"\"\n",
    "    A function to calculate normalized true regret for a batch\n",
    "\n",
    "    Args:\n",
    "        optmodel (optModel): optimization model\n",
    "        pred_cost (torch.tensor): predicted costs\n",
    "        true_cost (torch.tensor): true costs\n",
    "        true_obj (torch.tensor): true optimal objective values\n",
    "\n",
    "    Returns:predmodel\n",
    "        float: true regret losses\n",
    "    \"\"\"\n",
    "    # opt sol for pred cost\n",
    "    model = optmodel(pred_r, cost, Q, alpha=0.5)\n",
    "    sol, _ = model.solve()\n",
    "    # obj with true cost\n",
    "    obj = objValue(sol, true_r, alpha=0.5)\n",
    "    # loss\n",
    "    loss = true_obj - obj\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04270446719710229"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = regret(model, optmodel, loader_test)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def visLearningCurve(loss_log, loss_log_regret):\n",
    "    # create figure and subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,4))\n",
    "\n",
    "    # draw plot for training loss\n",
    "    ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve on Training Set\", fontsize=16)\n",
    "\n",
    "    # draw plot for regret on test\n",
    "    ax2.plot(loss_log_regret, color=\"royalblue\", ls=\"--\", alpha=0.7, lw=2)\n",
    "    ax2.set_xticks(range(0, len(loss_log_regret), 2))\n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve on Test Set\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "from pathos.multiprocessing import ProcessingPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class RegretLossFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, pred_r, true_r, cost, true_obj, optmodel):\n",
    "        device = pred_r.device\n",
    "\n",
    "        # Detach inputs to avoid modifying the original tensors\n",
    "        pred_r_np = pred_r.detach().cpu().numpy()\n",
    "        true_r_np = true_r.detach().cpu().numpy()\n",
    "        cost_np = cost.detach().cpu().numpy()\n",
    "        true_obj_np = true_obj.detach().cpu().numpy()\n",
    "\n",
    "        # Calculate the optimal solution for predicted costs\n",
    "        opt_solutions = []\n",
    "        opt_values = []\n",
    "        for i in range(pred_r_np.shape[0]):\n",
    "            optmodel = optmodel(pred_r_np[i], cost_np[i], Q, alpha=0.5)\n",
    "            sol, val = optmodel.solve()\n",
    "            opt_solutions.append(sol)\n",
    "            opt_values.append(val)\n",
    "\n",
    "        opt_solutions = np.array(opt_solutions)\n",
    "        opt_values = np.array(opt_values)\n",
    "\n",
    "        # Calculate the objective value with true costs\n",
    "        obj_values = []\n",
    "        for i in range(pred_r_np.shape[0]):\n",
    "            obj_val = objValue(opt_solutions[i], true_r_np[i], alpha=0.5)\n",
    "            obj_values.append(obj_val)\n",
    "\n",
    "        obj_values = np.array(obj_values)\n",
    "\n",
    "        # Calculate the regret loss\n",
    "        loss = true_obj_np - obj_values\n",
    "\n",
    "        # Save for backward pass\n",
    "        ctx.save_for_backward(pred_r, torch.FloatTensor(opt_solutions).to(device))\n",
    "        ctx.optmodel = optmodel\n",
    "\n",
    "        return torch.FloatTensor(loss).to(device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred_r, opt_solutions = ctx.saved_tensors\n",
    "\n",
    "        # Compute gradients using autograd\n",
    "        pred_r.requires_grad_(True)\n",
    "        with torch.enable_grad():\n",
    "            optmodel = ctx.optmodel\n",
    "            cost = pred_r.new_tensor(ctx.cost, requires_grad=True)\n",
    "            true_r = pred_r.new_tensor(ctx.true_r, requires_grad=True)\n",
    "            true_obj = pred_r.new_tensor(ctx.true_obj, requires_grad=True)\n",
    "            loss = RegretLossFunction.apply(pred_r, true_r, cost, true_obj, optmodel)\n",
    "\n",
    "        loss.backward()\n",
    "        grad_pred_r = pred_r.grad\n",
    "\n",
    "        return grad_pred_r, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regretLoss(optmodel, pred_r, true_r, cost, true_obj):\n",
    "    return RegretLossFunction.apply(pred_r, true_r, cost, true_obj, optmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = 0\n",
    "# for epoch in range(num_epochs):\n",
    " \n",
    "#     # start time\n",
    "#     tick = time.time()\n",
    "#     # load data\n",
    "#     for i, data in enumerate(loader_train):\n",
    "#         x, c, r, d, z  = data\n",
    "#         if torch.cuda.is_available():\n",
    "#             x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "#         rp = predmodel(x)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # train mode\n",
    "# predmodel.train()\n",
    "# # init log\n",
    "# loss_log = []\n",
    "# loss_log_regret = [regret(predmodel, optmodel, loader_test)]\n",
    "# # init elapsed time\n",
    "# elapsed = 0\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # start time\n",
    "#     tick = time.time()\n",
    "#     # load data\n",
    "#     for i, data in enumerate(loader_train):\n",
    "#         x, c, r, d, z  = data\n",
    "#         # cuda\n",
    "#         if torch.cuda.is_available():\n",
    "#             x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "#         # forward pass\n",
    "#         loss = torch.nn.MSELoss()\n",
    "#         # backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         # record time\n",
    "#         tock = time.time()\n",
    "#         elapsed += tock - tick\n",
    "#         # log\n",
    "#         loss_log.append(loss.item())\n",
    "#     regret_loss = regret(predmodel, optmodel, loader_test)\n",
    "#     loss_log_regret.append(regret_loss)\n",
    "#     print(\"Epoch {:2},  Loss: {:9.4f},  Regret: {:7.4f}%\".format(epoch+1, loss.item(), regret*100))\n",
    "# print(\"Total Elapsed Time: {:.2f} Sec.\".format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(predmodel, optmodel, loss_func, num_epochs=20, lr=1e-2):\n",
    "    # set adam optimizer\n",
    "    optimizer = torch.optim.Adam(predmodel.parameters(), lr=lr)\n",
    "    # train mode\n",
    "    predmodel.train()\n",
    "    # init log\n",
    "    loss_log = []\n",
    "    loss_log_regret = [regret(predmodel, optmodel, loader_test)]\n",
    "    # init elapsed time\n",
    "    elapsed = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # start time\n",
    "        tick = time.time()\n",
    "        # load data\n",
    "        for i, data in enumerate(loader_train):\n",
    "            x, c, r, d, z  = data\n",
    "            # cuda\n",
    "            if torch.cuda.is_available():\n",
    "                x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "            # forward pass\n",
    "            rp = predmodel(x)\n",
    "            for j in range(rp.shape[0]):\n",
    "                loss += loss_func(optmodel, rp[j], r[j], c[j], z[j])\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # record time\n",
    "            tock = time.time()\n",
    "            elapsed += tock - tick\n",
    "            # log\n",
    "            loss_log.append(loss.item())\n",
    "        regret_loss = regret(predmodel, optmodel, loader_test)\n",
    "        loss_log_regret.append(regret_loss)\n",
    "        print(\"Epoch {:2},  Loss: {:9.4f},  Regret: {:7.4f}%\".format(epoch+1, loss.item(), regret*100))\n",
    "    print(\"Total Elapsed Time: {:.2f} Sec.\".format(elapsed))\n",
    "\n",
    "    return loss_log, loss_log_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "trainModel() got multiple values for argument 'num_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m loss_log, loss_log_regret \u001b[38;5;241m=\u001b[39m trainModel(model, optmodel, criterion, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Visualize the learning curves\u001b[39;00m\n\u001b[0;32m     11\u001b[0m visLearningCurve(loss_log, loss_log_regret)\n",
      "\u001b[1;31mTypeError\u001b[0m: trainModel() got multiple values for argument 'num_epochs'"
     ]
    }
   ],
   "source": [
    "# Set up the model, optimizer, and MSE loss\n",
    "model = LinearRegressionModel(num_items, num_features).to(device)\n",
    "optmodel = optModel(r, c, Q, alpha=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Run the training\n",
    "loss_log, loss_log_regret = trainModel(model, optmodel, criterion, num_epochs=20, lr=1e-2)\n",
    "\n",
    "# Visualize the learning curves\n",
    "visLearningCurve(loss_log, loss_log_regret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
