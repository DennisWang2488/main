{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'/Users/dennis/Documents/myGit/main/myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    x = rnd.normal(0, 1, (n, m, p))\n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    \n",
    "    w = rnd.normal(0, 1, (m, p))\n",
    "    b = rnd.normal(0, 1, (n, m))\n",
    "    r = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            r[i, j] = np.dot(w[j], x[i, j]) + b[i, j]\n",
    "\n",
    "    r = 1 / (1 + np.exp(-r))\n",
    "\n",
    "    return x, r, c, Q\n",
    "\n",
    "class optModel:\n",
    "    \"\"\"\n",
    "    This is a class for optimization models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r, c, Q, alpha=0.5):\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data = num_data\n",
    "        self.num_items = num_items\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'optModel ' + self.__class__.__name__\n",
    "        \n",
    "\n",
    "    def setObj(self, r, c):\n",
    "\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "    \n",
    "    def solve(self, closed=False):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        self.d = cp.Variable(self.num_items)\n",
    "        self.setObj(self.r, self.c)\n",
    "        self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "        opt_sol = self.d.value\n",
    "        opt_val = self.problem.value\n",
    "\n",
    "        return opt_sol, opt_val\n",
    "\n",
    "\n",
    "    def solveC(self):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem in closed form for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        r = self.r\n",
    "        c = self.c\n",
    "        if self.alpha == 1:\n",
    "            raise ValueError(\"Work in progress\")\n",
    "        \n",
    "        S = np.sum(c ** (1 - 1 / self.alpha) * r ** (-1 + 1 / self.alpha))\n",
    "        opt_sol_c = (c ** (-1 / self.alpha) * r ** (-1 + 1 / self.alpha) * self.Q) / S\n",
    "        opt_val_c = np.sum((r * opt_sol_c) ** (1 - self.alpha)) / (1 - self.alpha)\n",
    "\n",
    "        return opt_sol_c, opt_val_c\n",
    "    \n",
    "class optDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is Torch Dataset class for optimization problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, costs, r, Q, alpha=0.5, closed=False):\n",
    "        \"\"\"\n",
    "        A method to create a optDataset from optModel\n",
    "\n",
    "        Args:\n",
    "            model (optModel): optimization model\n",
    "            features (np.ndarray): features\n",
    "            c (np.ndarray): c of objective function\n",
    "            r (np.ndarray): r of objective function\n",
    "            Q (float): budget\n",
    "            alpha (float): alpha of objective function\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        \"\"\"\n",
    "        self.feats = features\n",
    "        self.costs = costs\n",
    "        self.r = r\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.closed = closed\n",
    "\n",
    "        self.sols, self.objs = self._getSols()\n",
    "\n",
    "    def _getSols(self):\n",
    "        \"\"\"\n",
    "        A method to get the solutions of the optimization problem\n",
    "        \"\"\"\n",
    "        opt_sols = []\n",
    "        opt_objs = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.costs))):\n",
    "            sol, obj = self._solve(self.r[i], self.costs[i])\n",
    "            opt_sols.append(sol)\n",
    "            opt_objs.append([obj])\n",
    "        \n",
    "        return np.array(opt_sols), np.array(opt_objs)\n",
    "\n",
    "    def  _solve(self, r, c):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem to get oan optimal solution with given r and c\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): r of objective function\n",
    "            c (np.ndarray): c of objective function\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (np.ndarray), objective value (float)\n",
    "        \"\"\"\n",
    "        self.model = optModel(r, c, self.Q, self.alpha)\n",
    "        if self.closed:\n",
    "            return self.model.solveC()\n",
    "        else:\n",
    "            return self.model.solve()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        A method to get data size\n",
    "\n",
    "        Returns:\n",
    "            int: the number of optimization problems\n",
    "        \"\"\"\n",
    "        return len(self.costs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        A method to retrieve data\n",
    "\n",
    "        Args:\n",
    "            index (int): data index\n",
    "\n",
    "        Returns:\n",
    "            tuple: data features (torch.tensor), costs (torch.tensor), optimal solutions (torch.tensor) and objective values (torch.tensor)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            torch.FloatTensor(self.feats[index]), # x \n",
    "            torch.FloatTensor(self.costs[index]), # c\n",
    "            torch.FloatTensor(self.r[index]), # r \n",
    "            torch.FloatTensor(self.sols[index]),# optimal solution\n",
    "            torch.FloatTensor(self.objs[index]), # objective value\n",
    "        )\n",
    "    \n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, num_items, num_features):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_features, 1) for _ in range(num_items)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_items):\n",
    "            outputs.append(torch.sigmoid(self.linears[i](x[:, i, :])))\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "\n",
    "def regret(predmodel, optmodel, dataloader):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "    # load data\n",
    "    for data in dataloader:\n",
    "        x, c, r, d, z  = data\n",
    "        # cuda\n",
    "        if next(predmodel.parameters()).is_cuda:\n",
    "            x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            rp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "        # solve\n",
    "        for j in range(rp.shape[0]):\n",
    "            # accumulate loss\n",
    "            loss += calRegret(optModel, c[j].to(\"cpu\").detach().numpy(), rp[j], r[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "        optsum += abs(z).sum().item()\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return loss / (optsum + 1e-7)\n",
    "\n",
    "def objValue(d, r, alpha=0.5):\n",
    "    \"\"\"\n",
    "    A function to calculate objective value\n",
    "    \"\"\"\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(np.multiply(r, d)))\n",
    "    else:\n",
    "        return np.sum(np.power(np.multiply(r, d), 1 - alpha)) / (1 - alpha)\n",
    "\n",
    "\n",
    "\n",
    "def calRegret(optmodel, cost, pred_r, true_r, true_obj):\n",
    "    \"\"\"\n",
    "    A function to calculate normalized true regret for a batch\n",
    "\n",
    "    Args:\n",
    "        optmodel (optModel): optimization model\n",
    "        pred_cost (torch.tensor): predicted costs\n",
    "        true_cost (torch.tensor): true costs\n",
    "        true_obj (torch.tensor): true optimal objective values\n",
    "\n",
    "    Returns:predmodel\n",
    "        float: true regret losses\n",
    "    \"\"\"\n",
    "    # opt sol for pred cost\n",
    "    model = optmodel(pred_r, cost, Q, alpha=0.5)\n",
    "    sol, _ = model.solve()\n",
    "    # obj with true cost\n",
    "    obj = objValue(sol, true_r, alpha=0.5)\n",
    "    # loss\n",
    "    loss = true_obj - obj\n",
    "    return loss\n",
    "\n",
    "# Define the visualization function\n",
    "def visLearningCurve(loss_log, loss_log_regret):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "    ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve on Training Set\", fontsize=16)\n",
    "\n",
    "    ax2.plot(loss_log_regret, color=\"royalblue\", ls=\"--\", alpha=0.7, lw=2)\n",
    "    ax2.set_xticks(range(0, len(loss_log_regret), 2))\n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve on Test Set\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify visLearningCurve function\n",
    "def visLearningCurve(loss_log, loss_log_regret, mse_loss_log):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 4))\n",
    "\n",
    "    # Plot original loss log\n",
    "    ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve (Training Loss)\", fontsize=16)\n",
    "\n",
    "    # Plot regret log\n",
    "    ax2.plot(loss_log_regret, color=\"royalblue\", ls=\"--\", alpha=0.7, lw=2)\n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve (Test Regret)\", fontsize=16)\n",
    "\n",
    "    # Plot new MSE loss log\n",
    "    ax3.plot(mse_loss_log, color=\"orange\", lw=2)\n",
    "    ax3.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax3.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax3.set_ylabel(\"MSE Loss\", fontsize=16)\n",
    "    ax3.set_title(\"Learning Curve (MSE Loss)\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 200\n",
    "num_features = 20\n",
    "num_items = 10\n",
    "\n",
    "x, r, c, Q = genData(num_data, num_features, num_items)\n",
    "optmodel = optModel(r, c, Q, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 289.85it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 303.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test, r_train, r_test = train_test_split(x, c, r, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataset_train = optDataset(x_train, c_train, r_train, Q, alpha=0.5, closed=False)\n",
    "dataset_test = optDataset(x_test, c_test, r_test, Q, alpha=0.5, closed=False)\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class RegretLossFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, optmodel, cost, pred_r, true_r, true_obj, Q, alpha):\n",
    "        batch_size = pred_r.size(0)\n",
    "        losses = torch.zeros(batch_size, device=pred_r.device)\n",
    "        \n",
    "        pred_r_np = pred_r.detach().cpu().numpy()\n",
    "        true_r_np = true_r.detach().cpu().numpy()\n",
    "        cost_np = cost.detach().cpu().numpy()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # opt sol for pred cost\n",
    "            model = optmodel(pred_r_np[i], cost_np[i], Q, alpha=alpha)\n",
    "            sol, _ = model.solve()\n",
    "            # obj with true cost\n",
    "            obj = objValue(sol, true_r_np[i], alpha=alpha)\n",
    "\n",
    "            losses[i] = true_obj[i] - obj\n",
    "        \n",
    "        # Store necessary tensors for backward pass\n",
    "        ctx.save_for_backward(pred_r, true_r, cost, true_obj)\n",
    "        ctx.optmodel = optmodel\n",
    "        ctx.Q = Q\n",
    "        ctx.alpha = alpha\n",
    "        ctx.losses = losses\n",
    "        \n",
    "        return losses.mean().to(pred_r.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        pred_r, true_r, cost, true_obj = ctx.saved_tensors\n",
    "        optmodel = ctx.optmodel\n",
    "        Q = ctx.Q\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        batch_size = pred_r.size(0)\n",
    "        grad_pred_r = torch.zeros_like(pred_r)\n",
    "        \n",
    "        epsilon = 1e-5\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            pred_r_np = pred_r[i].detach().cpu().numpy()\n",
    "            cost_np = cost[i].detach().cpu().numpy()\n",
    "            true_r_np = true_r[i].detach().cpu().numpy()\n",
    "            \n",
    "            gradient = np.zeros_like(pred_r_np)\n",
    "            \n",
    "            for j in range(pred_r_np.size):\n",
    "                perturbed_pred_r = np.copy(pred_r_np)\n",
    "                perturbed_pred_r[j] += epsilon\n",
    "                \n",
    "                # Compute loss with perturbed pred_r\n",
    "                model = optmodel(perturbed_pred_r, cost_np, Q, alpha=alpha)\n",
    "                sol, _ = model.solve()\n",
    "                obj = objValue(sol, true_r_np, alpha=alpha)\n",
    "                loss_perturbed = true_obj[i] - obj\n",
    "                \n",
    "                # Finite difference approximation\n",
    "                gradient[j] = (loss_perturbed - ctx.losses[i].item()) / epsilon\n",
    "            \n",
    "            grad_pred_r[i] = torch.tensor(gradient, device=pred_r.device)\n",
    "        \n",
    "        grad_pred_r = grad_output.view(-1, 1) * grad_pred_r\n",
    "        \n",
    "        return None, None, grad_pred_r, None, None, None, None\n",
    "\n",
    "class RegretLoss(nn.Module):\n",
    "    def __init__(self, optmodel, Q, alpha=0.5):\n",
    "        super(RegretLoss, self).__init__()\n",
    "        self.optmodel = optmodel\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred_r, true_r, cost, true_obj):\n",
    "        return RegretLossFunction.apply(self.optmodel, cost, pred_r, true_r, true_obj, self.Q, self.alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(predmodel, loss_func, num_epochs=10, lr=1e-2):\n",
    "    optimizer = torch.optim.Adam(predmodel.parameters(), lr=lr)\n",
    "    mse_loss_func = nn.MSELoss()  # Fixed MSE loss metric\n",
    "    predmodel.train()\n",
    "    \n",
    "    loss_log = []\n",
    "    loss_log_regret = [regret(predmodel, optmodel, loader_test)]\n",
    "    mse_loss_log = []  # New MSE loss log\n",
    "    elapsed = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tick = time.time()\n",
    "        for i, data in enumerate(loader_train):\n",
    "            x, c, r, d, z = data\n",
    "            if torch.cuda.is_available():\n",
    "                x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            rp = predmodel(x)  # Forward pass\n",
    "\n",
    "            # Compute the regret loss (original loss)\n",
    "            loss = loss_func(rp, r, c, z)\n",
    "            # Compute MSE loss (new fixed metric)\n",
    "            mse_loss = mse_loss_func(rp, r)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tock = time.time()\n",
    "            elapsed += tock - tick\n",
    "            loss_log.append(loss.item())\n",
    "            mse_loss_log.append(mse_loss.item())  # Log the MSE loss\n",
    "\n",
    "        # Compute and log the regret loss after each epoch\n",
    "        regret_loss = regret(predmodel, optmodel, loader_test)\n",
    "        loss_log_regret.append(regret_loss)\n",
    "        print(f\"Epoch {epoch + 1:2},  Loss: {loss.item():9.4f},  MSE: {mse_loss.item():7.4f},  Regret: {regret_loss * 100:7.4f}%\")\n",
    "\n",
    "    print(f\"Total Elapsed Time: {elapsed:.2f} Sec.\")\n",
    "    return loss_log, loss_log_regret, mse_loss_log  # Return MSE loss log as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "Epoch  1,  Loss:    2.5304,  Regret: 13.3572%\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m RegretLoss(optModel, Q)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m loss_log, loss_log_regret \u001b[38;5;241m=\u001b[39m trainModel(model, criterion, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Visualize the learning curves\u001b[39;00m\n\u001b[0;32m     11\u001b[0m visLearningCurve(loss_log, loss_log_regret)\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(predmodel, loss_func, num_epochs, lr)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# loss = loss_func(rp, r) if using torch.nn.MSELoss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(rp, r, c, z)\n\u001b[1;32m---> 20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m tock \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up the model, optimizer, and MSE loss\n",
    "optmodel = optModel\n",
    "model = LinearRegressionModel(num_items, num_features).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = RegretLoss(optModel, Q)\n",
    "\n",
    "# Run the training\n",
    "loss_log, loss_log_regret = trainModel(model, criterion, num_epochs=10, lr=1e-2)\n",
    "\n",
    "# Visualize the learning curves\n",
    "visLearningCurve(loss_log, loss_log_regret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
