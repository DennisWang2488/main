{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd7a91c4570>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'/Users/dennis/Documents/myGit/main/myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    x = rnd.normal(0, 1, (n, m, p))\n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    \n",
    "    w = rnd.normal(0, 1, (m, p))\n",
    "    b = rnd.normal(0, 1, (n, m))\n",
    "    r = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            r[i, j] = np.dot(w[j], x[i, j]) + b[i, j]\n",
    "\n",
    "    r = 1 / (1 + np.exp(-r))\n",
    "\n",
    "    return x, r, c, Q\n",
    "\n",
    "class optModel:\n",
    "    \"\"\"\n",
    "    This is a class for optimization models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r, c, Q, alpha=0.5):\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data = num_data\n",
    "        self.num_items = num_items\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'optModel ' + self.__class__.__name__\n",
    "        \n",
    "\n",
    "    def setObj(self, r, c):\n",
    "\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "    \n",
    "    def solve(self, closed=False):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        self.d = cp.Variable(self.num_items)\n",
    "        self.setObj(self.r, self.c)\n",
    "        self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "        opt_sol = self.d.value\n",
    "        opt_val = self.problem.value\n",
    "\n",
    "        return opt_sol, opt_val\n",
    "\n",
    "\n",
    "    def solveC(self):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem in closed form for one set of parameters.\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): The r parameter for the optimization\n",
    "            c (np.ndarray): The c parameter for the optimization\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution and optimal value\n",
    "        \"\"\"\n",
    "        r = self.r\n",
    "        c = self.c\n",
    "        if self.alpha == 1:\n",
    "            raise ValueError(\"Work in progress\")\n",
    "        \n",
    "        S = np.sum(c ** (1 - 1 / self.alpha) * r ** (-1 + 1 / self.alpha))\n",
    "        opt_sol_c = (c ** (-1 / self.alpha) * r ** (-1 + 1 / self.alpha) * self.Q) / S\n",
    "        opt_val_c = np.sum((r * opt_sol_c) ** (1 - self.alpha)) / (1 - self.alpha)\n",
    "\n",
    "        return opt_sol_c, opt_val_c\n",
    "    \n",
    "class optDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is Torch Dataset class for optimization problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, costs, r, Q, alpha=0.5, closed=False):\n",
    "        \"\"\"\n",
    "        A method to create a optDataset from optModel\n",
    "\n",
    "        Args:\n",
    "            model (optModel): optimization model\n",
    "            features (np.ndarray): features\n",
    "            c (np.ndarray): c of objective function\n",
    "            r (np.ndarray): r of objective function\n",
    "            Q (float): budget\n",
    "            alpha (float): alpha of objective function\n",
    "            closed (bool): solving the problem in closed form\n",
    "\n",
    "        \"\"\"\n",
    "        self.feats = features\n",
    "        self.costs = costs\n",
    "        self.r = r\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.closed = closed\n",
    "\n",
    "        self.sols, self.objs = self._getSols()\n",
    "\n",
    "    def _getSols(self):\n",
    "        \"\"\"\n",
    "        A method to get the solutions of the optimization problem\n",
    "        \"\"\"\n",
    "        opt_sols = []\n",
    "        opt_objs = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.costs))):\n",
    "            sol, obj = self._solve(self.r[i], self.costs[i])\n",
    "            opt_sols.append(sol)\n",
    "            opt_objs.append([obj])\n",
    "        \n",
    "        return np.array(opt_sols), np.array(opt_objs)\n",
    "\n",
    "    def  _solve(self, r, c):\n",
    "        \"\"\"\n",
    "        A method to solve the optimization problem to get oan optimal solution with given r and c\n",
    "\n",
    "        Args:\n",
    "            r (np.ndarray): r of objective function\n",
    "            c (np.ndarray): c of objective function\n",
    "\n",
    "        Returns:\n",
    "            tuple: optimal solution (np.ndarray), objective value (float)\n",
    "        \"\"\"\n",
    "        self.model = optModel(r, c, self.Q, self.alpha)\n",
    "        if self.closed:\n",
    "            return self.model.solveC()\n",
    "        else:\n",
    "            return self.model.solve()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        A method to get data size\n",
    "\n",
    "        Returns:\n",
    "            int: the number of optimization problems\n",
    "        \"\"\"\n",
    "        return len(self.costs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        A method to retrieve data\n",
    "\n",
    "        Args:\n",
    "            index (int): data index\n",
    "\n",
    "        Returns:\n",
    "            tuple: data features (torch.tensor), costs (torch.tensor), optimal solutions (torch.tensor) and objective values (torch.tensor)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            torch.FloatTensor(self.feats[index]), # x \n",
    "            torch.FloatTensor(self.costs[index]), # c\n",
    "            torch.FloatTensor(self.r[index]), # r \n",
    "            torch.FloatTensor(self.sols[index]),# optimal solution\n",
    "            torch.FloatTensor(self.objs[index]), # objective value\n",
    "        )\n",
    "    \n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, num_items, num_features):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_features, 1) for _ in range(num_items)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_items):\n",
    "            outputs.append(torch.sigmoid(self.linears[i](x[:, i, :])))\n",
    "        return torch.cat(outputs, dim=1)\n",
    "    \n",
    "\n",
    "def regret(predmodel, optmodel, dataloader):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # evaluate\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "    # load data\n",
    "    for data in dataloader:\n",
    "        x, c, r, d, z  = data\n",
    "        # cuda\n",
    "        if next(predmodel.parameters()).is_cuda:\n",
    "            x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "        # predict\n",
    "        with torch.no_grad(): # no grad\n",
    "            rp = predmodel(x).to(\"cpu\").detach().numpy()\n",
    "        # solve\n",
    "        for j in range(rp.shape[0]):\n",
    "            # accumulate loss\n",
    "            loss += calRegret(optModel, c[j].to(\"cpu\").detach().numpy(), rp[j], r[j].to(\"cpu\").detach().numpy(),\n",
    "                              z[j].item())\n",
    "        optsum += abs(z).sum().item()\n",
    "    # turn back train mode\n",
    "    predmodel.train()\n",
    "    # normalized\n",
    "    return loss / (optsum + 1e-7)\n",
    "\n",
    "def objValue(d, r, alpha=0.5):\n",
    "    \"\"\"\n",
    "    A function to calculate objective value\n",
    "    \"\"\"\n",
    "    if alpha == 1:\n",
    "        return np.sum(np.log(np.multiply(r, d)))\n",
    "    else:\n",
    "        return np.sum(np.power(np.multiply(r, d), 1 - alpha)) / (1 - alpha)\n",
    "\n",
    "\n",
    "\n",
    "def calRegret(optmodel, cost, pred_r, true_r, true_obj):\n",
    "    \"\"\"\n",
    "    A function to calculate normalized true regret for a batch\n",
    "\n",
    "    Args:\n",
    "        optmodel (optModel): optimization model\n",
    "        pred_cost (torch.tensor): predicted costs\n",
    "        true_cost (torch.tensor): true costs\n",
    "        true_obj (torch.tensor): true optimal objective values\n",
    "\n",
    "    Returns:predmodel\n",
    "        float: true regret losses\n",
    "    \"\"\"\n",
    "    # opt sol for pred cost\n",
    "    model = optmodel(pred_r, cost, Q, alpha=0.5)\n",
    "    sol, _ = model.solve()\n",
    "    # obj with true cost\n",
    "    obj = objValue(sol, true_r, alpha=0.5)\n",
    "    # loss\n",
    "    loss = true_obj - obj\n",
    "    return loss\n",
    "\n",
    "# Define the visualization function\n",
    "def visLearningCurve(loss_log, loss_log_regret):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "    ax1.plot(loss_log, color=\"c\", lw=2)\n",
    "    ax1.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax1.set_xlabel(\"Iters\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=16)\n",
    "    ax1.set_title(\"Learning Curve on Training Set\", fontsize=16)\n",
    "\n",
    "    ax2.plot(loss_log_regret, color=\"royalblue\", ls=\"--\", alpha=0.7, lw=2)\n",
    "    ax2.set_xticks(range(0, len(loss_log_regret), 2))\n",
    "    ax2.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n",
    "    ax2.set_ylim(0, 0.5)\n",
    "    ax2.set_xlabel(\"Epochs\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Regret\", fontsize=16)\n",
    "    ax2.set_title(\"Learning Curve on Test Set\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 100\n",
    "num_features = 20\n",
    "num_items = 10\n",
    "\n",
    "x, r, c, Q = genData(num_data, num_features, num_items)\n",
    "optmodel = optModel(r, c, Q, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 145.09it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 138.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, c_train, c_test, r_train, r_test = train_test_split(x, c, r, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "dataset_train = optDataset(x_train, c_train, r_train, Q, alpha=0.5, closed=False)\n",
    "dataset_test = optDataset(x_test, c_test, r_test, Q, alpha=0.5, closed=False)\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class RegretLossFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, optmodel, cost, pred_r, true_r, true_obj, Q, alpha):\n",
    "        batch_size = pred_r.size(0)\n",
    "        losses = torch.zeros(batch_size, device=pred_r.device)\n",
    "        \n",
    "        pred_r_np = pred_r.detach().cpu().numpy()\n",
    "        true_r_np = true_r.detach().cpu().numpy()\n",
    "        cost_np = cost.detach().cpu().numpy()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # opt sol for pred cost\n",
    "            model = optmodel(pred_r_np[i], cost_np[i], Q, alpha=alpha)\n",
    "            sol, _ = model.solve()\n",
    "            # obj with true cost\n",
    "            obj = objValue(sol, true_r_np[i], alpha=alpha)\n",
    "\n",
    "            losses[i] = true_obj[i] - obj\n",
    "        \n",
    "        # Store necessary tensors for backward pass\n",
    "        ctx.save_for_backward(pred_r, true_r, cost, true_obj)\n",
    "        ctx.optmodel = optmodel\n",
    "        ctx.Q = Q\n",
    "        ctx.alpha = alpha\n",
    "        ctx.losses = losses\n",
    "        \n",
    "        return losses.mean().to(pred_r.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        pred_r, true_r, cost, true_obj = ctx.saved_tensors\n",
    "        optmodel = ctx.optmodel\n",
    "        Q = ctx.Q\n",
    "        alpha = ctx.alpha\n",
    "\n",
    "        batch_size = pred_r.size(0)\n",
    "        grad_pred_r = torch.zeros_like(pred_r)\n",
    "        \n",
    "        epsilon = 1e-5\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            pred_r_np = pred_r[i].detach().cpu().numpy()\n",
    "            cost_np = cost[i].detach().cpu().numpy()\n",
    "            true_r_np = true_r[i].detach().cpu().numpy()\n",
    "            \n",
    "            gradient = np.zeros_like(pred_r_np)\n",
    "            \n",
    "            for j in range(pred_r_np.size):\n",
    "                perturbed_pred_r = np.copy(pred_r_np)\n",
    "                perturbed_pred_r[j] += epsilon\n",
    "                \n",
    "                # Compute loss with perturbed pred_r\n",
    "                model = optmodel(perturbed_pred_r, cost_np, Q, alpha=alpha)\n",
    "                sol, _ = model.solve()\n",
    "                obj = objValue(sol, true_r_np, alpha=alpha)\n",
    "                loss_perturbed = true_obj[i] - obj\n",
    "                \n",
    "                # Finite difference approximation\n",
    "                gradient[j] = (loss_perturbed - ctx.losses[i].item()) / epsilon\n",
    "            \n",
    "            grad_pred_r[i] = torch.tensor(gradient, device=pred_r.device)\n",
    "        \n",
    "        grad_pred_r = grad_output.view(-1, 1) * grad_pred_r\n",
    "        \n",
    "        return None, None, grad_pred_r, None, None, None, None\n",
    "\n",
    "class RegretLoss(nn.Module):\n",
    "    def __init__(self, optmodel, Q, alpha=0.5):\n",
    "        super(RegretLoss, self).__init__()\n",
    "        self.optmodel = optmodel\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred_r, true_r, cost, true_obj):\n",
    "        return RegretLossFunction.apply(self.optmodel, cost, pred_r, true_r, true_obj, self.Q, self.alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(predmodel, loss_func, num_epochs=10, lr=1e-2):\n",
    "    optimizer = torch.optim.Adam(predmodel.parameters(), lr=lr)\n",
    "    predmodel.train()\n",
    "    loss_log = []\n",
    "    loss_log_regret = [regret(predmodel, optmodel, loader_test)]\n",
    "    elapsed = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tick = time.time()\n",
    "        for i, data in enumerate(loader_train):\n",
    "            x, c, r, d, z = data\n",
    "            if torch.cuda.is_available():\n",
    "                x, c, r, d, z = x.cuda(), c.cuda(), r.cuda(), d.cuda(), z.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            rp = predmodel(x)\n",
    "            # loss = loss_func(rp, r) if using torch.nn.MSELoss\n",
    "            loss = criterion(rp, r, c, z)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tock = time.time()\n",
    "            elapsed += tock - tick\n",
    "            loss_log.append(loss.item())\n",
    "\n",
    "        regret_loss = regret(predmodel, optmodel, loader_test)\n",
    "        loss_log_regret.append(regret_loss)\n",
    "        print(\"Epoch {:2},  Loss: {:9.4f},  Regret: {:7.4f}%\".format(epoch + 1, loss.item(), regret_loss * 100))\n",
    "\n",
    "    print(\"Total Elapsed Time: {:.2f} Sec.\".format(elapsed))\n",
    "    return loss_log, loss_log_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1,  Loss:    2.7178,  Regret: 13.0091%\n",
      "Epoch  2,  Loss:    2.5363,  Regret: 12.3445%\n",
      "Epoch  3,  Loss:    2.3086,  Regret: 11.6934%\n",
      "Epoch  4,  Loss:    2.0590,  Regret: 11.0971%\n",
      "Epoch  5,  Loss:    1.8805,  Regret: 10.5362%\n",
      "Epoch  6,  Loss:    1.6923,  Regret: 10.0247%\n",
      "Epoch  7,  Loss:    1.5645,  Regret:  9.5462%\n",
      "Epoch  8,  Loss:    1.5305,  Regret:  9.0965%\n"
     ]
    }
   ],
   "source": [
    "# Set up the model, optimizer, and MSE loss\n",
    "optmodel = optModel\n",
    "model = LinearRegressionModel(num_items, num_features).to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = RegretLoss(optModel, Q)\n",
    "\n",
    "# Run the training\n",
    "loss_log, loss_log_regret = trainModel(model, criterion, num_epochs=10, lr=1e-2)\n",
    "\n",
    "# Visualize the learning curves\n",
    "visLearningCurve(loss_log, loss_log_regret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
