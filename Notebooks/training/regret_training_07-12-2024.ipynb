{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Rewrite optModel and optDataset module\n",
    "2. Write a training loop\n",
    "3. Define regret and track regret/runtime\n",
    "4. Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test optModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d944cb0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'E:\\\\User\\\\Stevens\\\\Spring 2024\\\\PTO - Fairness\\\\myGit\\\\myUtils')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optModel:\n",
    "    def __init__(self, x, r, c, Q, alpha):\n",
    "        self.alpha = alpha\n",
    "        self.Q = Q\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.x = x\n",
    "        self.num_data, self.num_items, self.num_features = x.shape\n",
    "\n",
    "    def setObj(self, r, c):\n",
    "        if self.alpha == 1:\n",
    "            self.objective = cp.sum(cp.log(cp.multiply(r, self.d)))\n",
    "        else:\n",
    "            self.objective = cp.sum(cp.power(cp.multiply(r, self.d), 1 - self.alpha)) / (1 - self.alpha)\n",
    "        \n",
    "        self.constraints = [\n",
    "            self.d >= 0,\n",
    "            cp.sum(cp.multiply(c, self.d)) <= self.Q\n",
    "        ]\n",
    "        self.problem = cp.Problem(cp.Maximize(self.objective), self.constraints)\n",
    "\n",
    "    def solve(self, closed=False):\n",
    "        opt_sol = []\n",
    "        opt_val = []\n",
    "        if closed:\n",
    "            return self.solveC()\n",
    "\n",
    "        for i in range(self.num_data):\n",
    "            self.d = cp.Variable(self.num_items)\n",
    "            self.setObj(self.r[i], self.c[i])\n",
    "            self.problem.solve(abstol=1e-9, reltol=1e-9, feastol=1e-9)\n",
    "            opt_sol.append(self.d.value.reshape(1, self.num_items))\n",
    "            opt_val.append(self.problem.value)\n",
    "\n",
    "        opt_sol = np.concatenate(opt_sol)\n",
    "        return opt_sol, opt_val\n",
    "\n",
    "    def solveC(self):\n",
    "        if self.alpha == 1:\n",
    "            return \"Work in progress\"\n",
    "        \n",
    "        opt_sols_c = []\n",
    "        opt_vals_c = []\n",
    "        for i in range(self.num_data):\n",
    "            S = np.sum(self.c[i] ** (1 - 1 / self.alpha) * self.r[i] ** (-1 + 1 / self.alpha))\n",
    "            opt_sol_c = (self.c[i] ** (-1 / self.alpha) * self.r[i] ** (-1 + 1 / self.alpha) * self.Q) / S\n",
    "            opt_val_c = np.sum((self.r[i] * opt_sol_c) ** (1 - self.alpha)) / (1 - self.alpha)\n",
    "            opt_sols_c.append(opt_sol_c)\n",
    "            opt_vals_c.append(opt_val_c)\n",
    "        \n",
    "        opt_sols_c = np.array(opt_sols_c)\n",
    "        return opt_sols_c, opt_vals_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(num_data, num_features, num_items, seed=42, Q=100, dim=1, deg=1, noise_width=0.5, epsilon=0.1):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    n = num_data\n",
    "    p = num_features\n",
    "    m = num_items\n",
    "    \n",
    "    x = rnd.normal(0, 1, (n, m, p))\n",
    "    B = rnd.binomial(1, 0.5, (m, p))\n",
    "\n",
    "    c = np.zeros((n, m))\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            values = (np.dot(B[j], x[i, j].reshape(p, 1)).T / np.sqrt(p) + 3) ** deg + 1\n",
    "            values *= 5\n",
    "            values /= 3.5 ** deg\n",
    "            epislon = rnd.uniform(1 - noise_width, 1 + noise_width, 1)\n",
    "            values *= epislon\n",
    "            values = np.ceil(values)\n",
    "            c[i, j] = values\n",
    "\n",
    "    c = c.astype(np.float64)\n",
    "    r = rnd.normal(0, 1, (n, m))\n",
    "    r = 1 / (1 + np.exp(-r))\n",
    "\n",
    "    return x, r, c, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [ 3.70912192 13.14701444  3.06401269]\n",
      "Objective value: 12.051416435784253\n",
      "Optimal solution (closed form): [ 3.70911821 13.14703314  3.06400571]\n",
      "Objective value (closed form): 12.051416436072254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test optModel with synthetic data\n",
    "x, r, c, Q = genData(100, 4, 3)\n",
    "alpha = 0.5\n",
    "\n",
    "# Create an instance of the optModel class\n",
    "optmodel = optModel(x, r, c, Q, alpha)\n",
    "\n",
    "# Solve the optimization problem\n",
    "opt_sol, opt_val = optmodel.solve()\n",
    "\n",
    "print(\"Optimal solution:\", opt_sol[0])\n",
    "print(\"Objective value:\", opt_val[0])\n",
    "\n",
    "opt_sol_c, opt_val_c = optmodel.solve(closed=True)\n",
    "\n",
    "print(\"Optimal solution (closed form):\", opt_sol_c[0])\n",
    "print(\"Objective value (closed form):\", opt_val_c[0])\n",
    "\n",
    "# Are they the same?\n",
    "np.allclose(opt_sol, opt_sol_c, atol=1e-4, rtol=1e-4)\n",
    "np.allclose(opt_val, opt_val_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试optDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optDataset(Dataset):\n",
    "    def __init__(self, x, r, c, Q, alpha, closed=False):\n",
    "        self.closed = closed\n",
    "        self.x = x\n",
    "        self.r = r\n",
    "        self.c = c\n",
    "        self.Q = Q\n",
    "        self.alpha = alpha\n",
    "        self.num_data, self.num_items, self.num_features = x.shape\n",
    "\n",
    "        self._solve_optimization_problems()\n",
    "\n",
    "    def _solve_optimization_problems(self):\n",
    "        optmodel = optModel(self.x, self.r, self.c, self.Q, self.alpha)\n",
    "        self.opt_sols, self.opt_vals = optmodel.solve()\n",
    "        self.opt_sols_c, self.opt_vals_c = optmodel.solve(closed=True)\n",
    "        self.opt_vals = np.array(self.opt_vals)\n",
    "        self.opt_vals_c = np.array(self.opt_vals_c)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.r)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ret = (\n",
    "                torch.tensor(self.x[idx]),\n",
    "                torch.tensor(self.r[idx]),\n",
    "                torch.tensor(self.c[idx]),\n",
    "                torch.tensor(self.opt_sols[idx]),\n",
    "                torch.tensor(self.opt_vals[idx]),\n",
    "                \n",
    "            )\n",
    "        if self.closed:\n",
    "            ret = ret + (torch.tensor(self.opt_sols_c[idx]),\n",
    "                torch.tensor(self.opt_vals_c[idx]),\n",
    "    )                \n",
    "        return ret\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4967, -0.1383,  0.6477,  1.5230],\n",
       "         [-0.2342, -0.2341,  1.5792,  0.7674],\n",
       "         [-0.4695,  0.5426, -0.4634, -0.4657]], dtype=torch.float64),\n",
       " tensor([0.6599, 0.7638, 0.5451], dtype=torch.float64),\n",
       " tensor([7., 4., 7.], dtype=torch.float64),\n",
       " tensor([ 3.7091, 13.1470,  3.0640], dtype=torch.float64),\n",
       " tensor(12.0514, dtype=torch.float64))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test optDatasetRd\n",
    "dataset = optDataset(x, r, c, Q, alpha)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 3\n",
      "Number of features: 4\n"
     ]
    }
   ],
   "source": [
    "num_items = x.shape[1]\n",
    "num_features = x.shape[2]\n",
    "print(\"Number of items:\", num_items)\n",
    "print(\"Number of features:\", num_features)\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, num_items, num_features):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.linears = nn.ModuleList([nn.Linear(num_features, 1) for _ in range(num_items)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.num_items):\n",
    "            outputs.append(torch.sigmoid(self.linears[i](x[:, i, :])))\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRegret(optmodel, x, true_c, pred_r, true_r, true_obj, alpha):\n",
    "    model = optmodel(x,pred_r, true_c, Q, alpha)\n",
    "    sol, _ = model.solve()\n",
    "    val = []\n",
    "    for i in range(x.shape[1]):\n",
    "        temp = np.sum((true_r[i] * sol[i]) ** (1 - alpha)) / (1 - alpha)\n",
    "        val.append(temp)\n",
    "    val = torch.tensor(np.array(val)).to(device)\n",
    "    regret_loss = 0\n",
    "    for i in range(x.shape[1]):\n",
    "        regret_loss += true_obj[i] - val[i]\n",
    "    return regret_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "model = LogisticRegressionModel(num_items, num_features).to(device)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linears): ModuleList(\n",
       "    (0-2): 3 x Linear(in_features=4, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m regret_loss \u001b[38;5;241m=\u001b[39m calRegret(optModel, x_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), c_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), pred_r, r_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), opt_vals_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), alpha)\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m regret_loss\n\u001b[1;32m---> 34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     37\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop with custom loss (regret)\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x_batch, r_batch, c_batch, opt_sols_batch, opt_vals_batch in loader_train:\n",
    "        \n",
    "        x_batch = x_batch.float().to(device)\n",
    "        r_batch = r_batch.float().to(device)\n",
    "        c_batch = c_batch.float().to(device)\n",
    "        opt_vals_batch = opt_vals_batch.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            pred_r = model(x_batch).to(\"cpu\").detach().numpy()\n",
    "        \n",
    "        regret_loss = calRegret(optModel, x_batch.detach().cpu().numpy(), c_batch.detach().cpu().numpy(), pred_r, r_batch.detach().cpu().numpy(), opt_vals_batch.detach().cpu().numpy(), alpha)\n",
    "        loss = regret_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(loader_train):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(predmodel, optmodel, dataloader, closed=False, alpha=0.5):\n",
    "    \"\"\"\n",
    "    A function to evaluate model performance with normalized true regret\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn): a regression neural network for cost prediction\n",
    "        optmodel (optModel): an PyEPO optimization model\n",
    "        dataloader (DataLoader): Torch dataloader from optDataSet\n",
    "\n",
    "    Returns:\n",
    "        float: true regret loss\n",
    "    \"\"\"\n",
    "    # eval\n",
    "    predmodel.eval()\n",
    "    loss = 0\n",
    "    optsum = 0\n",
    "\n",
    "    if not closed:\n",
    "    # load data\n",
    "        for data in dataloader:\n",
    "            x,r,c,opt_sol,opt_obj,_,_ = data\n",
    "            # cuda\n",
    "            x,r,c,opt_sol,opt_obj = x.cuda(),r.cuda(),c.cuda(),opt_sol.cuda(),opt_obj.cuda()\n",
    "            # predict\n",
    "            with torch.no_grad():\n",
    "                pred_r = predmodel(x).to('cpu').detach().numpy()\n",
    "            # solve\n",
    "            for j in range(pred_r.shape[0]):\n",
    "                loss += calRegret(optmodel, c[j], pred_r[j], r[j].to(\"cpu\").detach().numpy(), opt_obj[j].item(), alpha)\n",
    "\n",
    "                optsum += abs(opt_obj[j].item())\n",
    "    # turn back to train mode\n",
    "    predmodel.train()\n",
    "\n",
    "    # normalize\n",
    "    return loss / (optsum+1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
