{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end learning with closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-stage Method vs. e2e with fair constraints vs. fair objectives**\n",
    "\n",
    "1. Create a dataset\n",
    "\n",
    "2. Optimization model\n",
    "\n",
    "4. Training\n",
    "    - with different loss functions\n",
    "\n",
    "5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x9c7a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.printoptions(suppress=True)\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.insert(1,'E:\\\\User\\\\Stevens\\\\Spring 2024\\\\PTO - Fairness\\\\myUtils')\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "from genData import genData\n",
    "from optDataset import AlphaFairOptDataset\n",
    "from optModel import optCvModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret_loss(predmodel, data, device):\n",
    "    \"\"\"\n",
    "    A function to calculate regret loss.\n",
    "\n",
    "    Args:\n",
    "        predmodel (nn.Module): a regression neural network for cost prediction\n",
    "        data (tuple): a batch of data from the dataloader\n",
    "        device (torch.device): device to perform computation on\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: calculated regret loss\n",
    "    \"\"\"\n",
    "    x, _, _, r_true = data\n",
    "    x = x.float().to(device)\n",
    "    r_true = r_true.float().to(device)\n",
    "\n",
    "    # Predict r_hat\n",
    "    r_hat = predmodel(x)\n",
    "\n",
    "    total_regret = torch.tensor(0.0, device=device)\n",
    "    total_opt_value = torch.tensor(0.0, device=device)\n",
    "\n",
    "    for i in range(x.size(0)):\n",
    "        optmodel = optCvModel(\n",
    "            n=a.shape[1],\n",
    "            alpha=0.5,\n",
    "            Q=Q,\n",
    "            epsilon=epsilon,\n",
    "            a=a[i],\n",
    "            r=r[i],\n",
    "            b=b[i],\n",
    "            c=c[i]\n",
    "        )\n",
    "        # Predicted values\n",
    "        optmodel.setObj(a[i], r_hat[i].detach().cpu().numpy(), b[i], c[i])\n",
    "        _, _, opt_value_pred = optmodel.solveP()\n",
    "        \n",
    "        # True values\n",
    "        optmodel.setObj(a[i], r_true[i].cpu().numpy(), b[i], c[i])\n",
    "        _, _, opt_value_true = optmodel.solve()\n",
    "        \n",
    "        # Calculate regret\n",
    "        total_regret += torch.abs(torch.tensor(opt_value_true - opt_value_pred, device=device))\n",
    "        total_opt_value += torch.abs(torch.tensor(opt_value_true, device=device))\n",
    "    \n",
    "    regret = total_regret / (total_opt_value + 1e-7)\n",
    "    return regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 1000\n",
    "num_features = 10\n",
    "num_items = 5\n",
    "a, b, c, r, x, Q, epsilon = genData(num_data, num_features, num_items)\n",
    "\n",
    "# Create dataset\n",
    "dataset = AlphaFairOptDataset(a, b, c, r, x, Q, alpha=0.5, epsilon=epsilon)\n",
    "\n",
    "# Train-test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a simple logistic regression model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_items)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "input_dim = num_features\n",
    "model = LogisticRegressionModel(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel(\n",
       "  (linear): Linear(in_features=10, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m regret_loss(model, data, device)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\14469\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for data in loader_train:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate regret loss\n",
    "        # loss = regret_loss(model, data, device)\n",
    "        loss = criterion(r_hat, r_true)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    train_losses.append(epoch_loss / len(loader_train))\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(loader_train):.4f}')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve using Regret')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error element 0 of tensors does not require grad and does not have a grad_fn indicates that the tensor you're trying to backpropagate through does not have requires_grad=True. This typically happens when a tensor is created or manipulated in such a way that it loses its gradient tracking.\n",
    "\n",
    "In our scenario, the issue arises because the regret calculation involves operations that do not maintain gradient tracking. Specifically, the values passed to optmodel.setObj and the calculations within the optCvModel do not support gradient tracking since they involve numpy operations and optimizations that aren't differentiable.\n",
    "\n",
    "To address this, we need to ensure that our regret calculation is compatible with PyTorch's autograd system. Since the optimization problem involves non-differentiable operations, we should look into alternative methods such as differentiable surrogates or reinforcement learning approaches.\n",
    "\n",
    "However, a practical way to move forward within the scope of this example is to manually compute gradients using a surrogate loss function that is differentiable. Here, we simplify the regret loss to focus on the PyTorch components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Regret: 0.06345776751992198\n",
      "Function-based Regret: 0.06345776751992198\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generate synthetic data for a single sample\n",
    "num_data = 1\n",
    "num_features = 10\n",
    "num_items = 5\n",
    "a, b, c, r, x, Q, epsilon = genData(num_data, num_features, num_items)\n",
    "\n",
    "# Create dataset for a single sample\n",
    "single_sample_dataset = AlphaFairOptDataset(a, b, c, r, x, Q, alpha=0.5, epsilon=epsilon)\n",
    "single_sample_loader = DataLoader(single_sample_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define a simple logistic regression model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_items)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Initialize model and transfer to device\n",
    "input_dim = num_features\n",
    "model = LogisticRegressionModel(input_dim).to(device)\n",
    "\n",
    "# Function to manually calculate regret for a single sample\n",
    "def manual_regret_calculation(optmodel, a, b, c, r, Q, epsilon, r_hat):\n",
    "    optmodel.setObj(a, r_hat, b, c)\n",
    "    _, _, opt_value_pred = optmodel.solveP()\n",
    "    optmodel.setObj(a, r, b, c)\n",
    "    _, _, opt_value_true = optmodel.solve()\n",
    "    total_regret = abs(opt_value_true - opt_value_pred)\n",
    "    total_opt_value = abs(opt_value_true)\n",
    "    return total_regret / (total_opt_value + 1e-7)\n",
    "\n",
    "# Testing the regret function with one-sample test case\n",
    "def test_regret_function():\n",
    "    model.eval()\n",
    "    total_regret = 0.0\n",
    "    total_opt_value = 0.0\n",
    "\n",
    "    for data in single_sample_loader:\n",
    "        x, _, _, _ = data\n",
    "        x = x.float().to(device)\n",
    "        with torch.no_grad():\n",
    "            r_hat = model(x).cpu().numpy()\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            optmodel = optCvModel(\n",
    "                n=a.shape[1],\n",
    "                alpha=0.5,\n",
    "                Q=Q,\n",
    "                epsilon=epsilon,\n",
    "                a=a[i],\n",
    "                r=r[i],\n",
    "                b=b[i],\n",
    "                c=c[i]\n",
    "            )\n",
    "            # Manual regret calculation\n",
    "            manual_regret = manual_regret_calculation(optmodel, a[i], b[i], c[i], r[i], Q, epsilon, r_hat[i])\n",
    "            \n",
    "            # Function-based regret calculation\n",
    "            optmodel.setObj(a[i], r_hat[i], b[i], c[i])\n",
    "            _, _, opt_value_pred = optmodel.solveP()\n",
    "            optmodel.setObj(a[i], r[i], b[i], c[i])\n",
    "            _, _, opt_value_true = optmodel.solve()\n",
    "            func_regret = abs(opt_value_true - opt_value_pred) / (abs(opt_value_true) + 1e-7)\n",
    "\n",
    "            print(f'Manual Regret: {manual_regret}')\n",
    "            print(f'Function-based Regret: {func_regret}')\n",
    "\n",
    "            assert np.isclose(manual_regret, func_regret), \"Regret calculation mismatch!\"\n",
    "\n",
    "# Generate synthetic weights for the model (this would normally come from training)\n",
    "with torch.no_grad():\n",
    "    model.linear.weight = nn.Parameter(torch.randn(num_items, num_features).to(device))\n",
    "    model.linear.bias = nn.Parameter(torch.randn(num_items).to(device))\n",
    "\n",
    "# Test the regret function\n",
    "test_regret_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
